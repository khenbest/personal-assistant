#!/bin/bash

# Setup script for Ollama models
# Installs the lightweight models needed for intent classification

echo "ðŸš€ Kenny's Personal Assistant - Ollama Setup"
echo "==========================================="
echo ""

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama is not installed!"
    echo ""
    echo "Please install Ollama first:"
    echo "  brew install ollama"
    echo "  OR"
    echo "  Download from: https://ollama.ai"
    echo ""
    exit 1
fi

echo "âœ… Ollama is installed"
echo ""

# Check if Ollama is running
if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âš ï¸  Ollama is not running. Starting it now..."
    ollama serve > /dev/null 2>&1 &
    sleep 3
    
    if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âŒ Failed to start Ollama. Please run 'ollama serve' manually."
        exit 1
    fi
fi

echo "âœ… Ollama is running"
echo ""

# Function to check and pull models
pull_model() {
    local model=$1
    local size=$2
    local description=$3
    
    echo "ðŸ“¦ Checking $model..."
    
    # Check if model exists
    if ollama list | grep -q "$model"; then
        echo "   âœ… $model is already installed"
    else
        echo "   â¬‡ï¸  Downloading $model ($size) - $description"
        echo "   This may take a few minutes..."
        
        if ollama pull "$model"; then
            echo "   âœ… $model installed successfully!"
        else
            echo "   âŒ Failed to install $model"
            return 1
        fi
    fi
    echo ""
}

echo "ðŸ“¥ Installing required models..."
echo "================================"
echo ""

# Install primary model - Qwen2.5 1.5B (fastest, smallest)
pull_model "qwen2.5:1.5b" "0.9GB" "Ultra-fast primary model for intent classification"

# Install fallback model - Llama3.2 3B (more capable)
pull_model "llama3.2:3b" "2.0GB" "More capable fallback model"

echo ""
echo "ðŸ”¥ Warming up models..."
echo "========================"

# Warm up Qwen
echo "Testing qwen2.5:1.5b..."
echo "Hi" | ollama run qwen2.5:1.5b --verbose 2>/dev/null | head -1
echo "âœ… qwen2.5:1.5b ready"

# Warm up Llama
echo "Testing llama3.2:3b..."
echo "Hi" | ollama run llama3.2:3b --verbose 2>/dev/null | head -1
echo "âœ… llama3.2:3b ready"

echo ""
echo "ðŸ“Š Model Summary"
echo "================"
ollama list | grep -E "qwen2.5:1.5b|llama3.2:3b"

echo ""
echo "âœ¨ Setup complete!"
echo ""
echo "ðŸŽ¯ Next steps:"
echo "   1. Run the test: npm run test:ollama"
echo "   2. Start the backend: npm run dev"
echo "   3. Test intent classification locally!"
echo ""
echo "ðŸ’¡ Tips:"
echo "   - Qwen2.5:1.5b will handle most requests (<100ms response)"
echo "   - Llama3.2:3b will activate for complex queries"
echo "   - No internet required after setup!"
echo "   - Total disk usage: ~3GB"
echo ""