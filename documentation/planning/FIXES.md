Full Codebase Audit and Optimization Plan for Personal Assistant App
Overview of Current Architecture and Challenges
The Personal Assistant app is a cross-platform system composed of a Node.js backend (Fastify server), a React Native mobile app (Expo), a CLI tool, and voice input integration. Its core feature is natural language intent classification for user commands (e.g. scheduling events, reminders, notes, emails). Currently, the system achieves ~85% accuracy using an LLM-based classifier with rule-based helpers, and it incorporates a feedback loop where user corrections are stored for learning. Key components include:
IntentService (Backend) – Uses a Large Language Model (via UnifiedLLMService) for intent classification (~85% accuracy) and a kNN lookup for exact matches from past corrections.
SlotExtractionService (Backend) – Extracts structured data (“slots” like dates, emails, durations) using chrono-node for dates, WinkNLP for entity recognition, plus intent-specific rules and LLM fallback for ambiguous cases.
Mobile App (React Native) – Sends user queries to the backend for classification, then either executes the intent on-device (e.g. create a calendar event) or prompts the user for confirmation via a Correction UI if confidence is low.
CLI Interface – A command-line tool that sends queries to the same API and prints the classified intent and slots for development and testing.
Voice Integration – Allows voice commands by recording audio, transcribing to text, then sending to the classifier. If confident, it executes the intent and uses text-to-speech for output.
Data Storage (Supabase) – A Postgres database (hosted on Supabase) stores intent predictions and user corrections for analytics and continuous learning. It also stores embeddings for semantic search (pgvector) to enable kNN-based intent matching.
Current challenges: The rapid implementation (in three sprints) introduced some duplication and complexity in the codebase. Multiple NLP approaches (LLM prompts, rule-based extraction, upcoming TF.js model, embedding search) need better unification. Ensuring consistency across backend, mobile, CLI, and voice layers is vital. We aim to optimize each area – backend services, mobile integration, CLI, voice handling, data architecture, and LLM/ML pipeline – for maintainability, performance, and scalability. Below, we audit each component and provide detailed optimization recommendations.
Backend Services Audit & Optimization
Current State: The backend (in packages/backend) exposes REST endpoints for intent classification and learning. The IntentService class encapsulates intent detection using the LLM and correction lookup. After Sprint 1, it achieved ≥85% accuracy on five core intents. By design, it supports storing new corrections (via /api/intent/correction) to improve future predictions. A new SlotExtractionService was integrated to parse dates, emails, durations, etc., combining chrono-node and WinkNLP with possible LLM fallback. The backend also has a UnifiedLLMService (for calling OpenAI or similar) and, by Sprint 3, a plan for a TensorFlow.js or ONNX Runtime model to handle classification locally for speed. A caching layer was introduced to speed up repeated queries (with 1-hour TTL and persistent storage of very high-confidence results). Findings & Issues: Overall, the backend code is well-structured into services and routes. However, the integration of multiple classification methods can be improved:
The code currently checks for exact text matches (kNN or direct lookup) to apply user corrections, but as we add embeddings and a local model, the logic may become scattered or duplicated. We see the plan for a unified pipeline: (1) cache, (2) kNN search, (3) TF.js model, (4) LLM fallback. This pipeline should be cleanly implemented to avoid code duplication and ensure each step is only executed when needed.
The IntentService can be refactored to orchestrate this pipeline. For example, on classifyIntent(text): first check the in-memory cache (log “Cache hit” as in current code), then query the embeddings/pgvector index for similar past queries, then use the TF.js model if available, and only then call the LLM as last resort. Right now, some of these steps are implied in docs but must be implemented in code. Ensuring this sequence is modular (perhaps by splitting into helper methods or strategy classes) will improve maintainability.
Performance: The caching introduced (with O(1000) size limit) is a good optimization. We should ensure the cache is used effectively (e.g., normalize text case as done in code, possibly strip punctuation) to maximize hits. Also, consider caching at the HTTP layer (Fastify) for identical requests within a short window, if applicable. The LLM calls are the slowest (~300-500ms each), so cache and the new lightweight model should dramatically reduce average latency.
Concurrency & Resource Use: The backend must handle concurrent requests. Fastify and Node’s async model can handle dozens of requests (target 50+ concurrent as per Sprint 3 metrics). We should review if the UnifiedLLMService or TF.js model loading is done once at server start or per request. The code snippet shows instantiating UnifiedLLMService inside the route handler registration – that likely creates a new instance for each route registration (which is fine, since registration happens on startup). We should reuse expensive instances if possible (e.g., the embedding model pipeline can be loaded once and reused). An optimization is to initialize the ML models at server startup and inject them into services, rather than reloading each time.
Error Handling: Ensure all promises have .catch blocks or are inside try-catch (the scripts and services appear to catch errors in tests). The API should return meaningful errors (the code currently throws generic 500 if classification fails). We can improve by catching known issues (e.g., Supabase connection failure, LLM timeout) and returning a user-friendly message or fallback result. Common issues are documented (e.g., Supabase key misconfigurations) – these should be asserted on startup to fail fast if config is wrong.
Code Cleanup: Remove any deprecated logic. For example, if an older rule-based intent classifier (prior to LLM) exists, it should be deleted or fully integrated as fallback. From the documentation, it appears the LLM approach replaced older methods entirely, so likely no unused code, but verify for any commented-out blocks or alternative services that are no longer used.
Testing: The backend has a comprehensive test suite for classification and slot extraction. Continue to expand these tests, especially as new methods (embedding search, TF.js model) are added. We should include tests for the cache hit path, the kNN path, etc., to ensure each yields consistent classification results. End-to-end tests were created (Day 3 E2E test script) to verify overall accuracy against a dataset – these should be updated as the pipeline evolves to ensure the 90% and 95% accuracy targets are met.
Future Backend Improvements: After integrating the TF.js/ONNX model (for ~95% accuracy and <100ms inference), consider an A/B testing mechanism where a percentage of requests still go through the LLM even if the model is confident, to continually validate the model’s predictions against the more powerful LLM. This can help detect drift or corner cases (as hinted by plans for an A/B testing framework). Also plan periodic re-training or fine-tuning of the local model using new data (perhaps offline, since continuous training on-device is out of scope; instead, collect data and retrain offline then deploy an updated model).
By optimizing the backend in these ways, we ensure a robust, fast classification service that cleanly integrates caching, vector search, and model inference with minimal duplicated logic, providing a solid foundation for all client interfaces.
Mobile App Integration Audit & Optimization
Current State: The React Native mobile app (in packages/mobile) primarily serves as the UI and execution layer for the assistant. It sends the user’s text input to the backend and receives the classified intent and extracted slots. If the prediction confidence is low or ambiguous, the app presents a Correction UI to the user, allowing them to confirm or correct the intent before execution. The Correction UI component (CorrectionUI.tsx) displays the original text and the predicted intent/slots, and lets the user either accept or provide the correct intent/slots. When the user submits a correction, the app calls the /api/intent/correction endpoint to record it, then proceeds to execute the corrected intent. For executing intents on device, the app includes service modules for device-specific actions. For example:
CalendarService handles creating calendar events using Expo’s Calendar API.
Reminder/Notification Service for reminders (integrating with iOS Reminders or local notifications).
Note creation might just save to a local store or send to backend (not shown, but an executeIntent method stub exists).
Email sending/reading might integrate with an email API or be a no-op stub for now.
Findings & Issues: The mobile app effectively extends the backend’s functionality to the device, but a few areas need attention:
Duplication of Logic: Ideally, the mobile should not replicate any NLP logic that the backend handles. We see that the mobile installed chrono-node, likely for date parsing. This was done because WinkNLP had issues on React Native. Currently, the app likely relies on the backend’s result for slots (the IntentResult from the API includes slots like datetime, title, etc.), so the mobile might not actually need to run chrono-node itself. It could be a vestigial installation or used for minor local adjustments. Optimization: Keep all heavy NLP (intent classification and full slot extraction) on the backend or the shared ML model, to avoid inconsistent results. The mobile should simply display what the backend (or future on-device model) returns. If the app currently does any parsing (e.g., to format dates or pick out email addresses for display), ensure it uses the same logic as backend. For example, if the backend returns an ISO datetime, the mobile can format it for display or confirmation but should not try to re-parse natural language on its own.
Offline Mode Preparation: A key goal is to achieve offline and low-latency operation by integrating a TensorFlow.js or ONNX model for on-device inference. This means the mobile app could classify intents without a network call, once the model is available (Sprint 3). We need to plan for this: if offline mode is enabled, the app should load the small model (likely via tfjs-react-native or a compiled model file) and use it to classify user input locally. The pipeline in offline mode would be: user speech/text → local model → (if low confidence) maybe call cloud LLM as fallback when online, or ask user for clarification. Implementing this will require:
Distributing the model with the app (keeping it <5MB as targeted).
Ensuring the mobile app can use a JavaScript or WASM model efficiently (consider using ONNX Runtime for React Native for better performance if available, or TensorFlow Lite via a native module as an alternative).
Abstracting the classification API: currently apiService.classifyIntent() always calls the server. We could modify APIService to choose between local model vs server based on connectivity or a setting. E.g., if(offlineModeEnabled) { result = LocalModel.classify(text) } else { result = await fetch(server...) }.
Thorough testing on device for performance (<100ms inference target on modern phones) and memory impact (ensure it doesn’t crash low-end devices).
UI/UX Improvements: The Correction UI is a crucial part of the feedback loop. We should ensure it is user-friendly: e.g., showing the confidence level or “I’m not sure” message when triggering it (the code currently triggers if confidence < 0.85 or an explicit needsConfirmation flag). We might allow voice feedback in this step too (for example, if using voice and confidence is low, have the assistant voice ask for clarification). Another UI consideration is to avoid interrupting the flow: perhaps allow the user to ignore the suggestion and proceed anyway (“onConfirm” executes the intent as-is).
Maintainability: The mobile app defines specific handling for each intent in executeIntent (using a switch-case). As the number of intents grows, this could become unwieldy. We should consider a more scalable design:
Use an intent registry (perhaps the intents_registry.json mentioned) to define what each intent does. On mobile, this could map intent names to handler functions or component actions. This way adding a new intent is a matter of extending the registry in one place rather than editing multiple switch statements.
Ensure that both backend and mobile refer to the same source of truth for supported intents and slot definitions. The presence of a JSON registry suggests we can share that config.
If some intents are meant to be handled purely on backend (e.g., “read_email” might require server-side integration with an email service or database), the mobile should know to call a backend endpoint rather than handle locally. Conversely, for intents like “create_event” that use device APIs, the backend just returns the classification and the mobile does the rest. Documenting which side handles which intent is important to avoid confusion. In code, perhaps the backend’s executeIntent API simply proxies to mobile via notifications – but currently it’s all client-driven. This is fine, just needs clarity.
Performance: The mobile app’s performance mostly depends on network calls to the backend. To optimize perceived latency, we can implement optimistic UI updates. For example, after the user submits a command, show a quick acknowledgment (e.g., a spinner or “Understanding...”) while awaiting the classification. If the model moves on-device, the response will be faster, but until then, mitigate the delay with good UX. Also, ensure that for actions like creating calendar events or reminders, any slow operations (like asking for permissions the first time) are handled gracefully (prompt user, etc.). Caching frequent operations on device (like caching the default calendar ID after first retrieval to avoid repeatedly calling Calendar.getCalendarsAsync()) is another micro-optimization.
Testing: Implement UI tests for the Correction flow (e.g., using Jest/React Native Testing Library or manual testing scenarios). Verify that when needsConfirmation is true, the UI shows up and that providing a correction actually results in a new execution with the corrected intent. Also test the mobile integration with backend thoroughly: scenarios where the backend is unreachable (network down) – does the app handle it gracefully (right now an exception from fetch would throw; we might want a user-facing error message “Unable to reach server”)? This becomes even more important as we promise offline capabilities – the app should not just fail silently if offline; it should detect offline mode and switch to local processing if possible.
By addressing the above, the mobile app will be prepared to seamlessly work with the evolving backend/ML pipeline. The result will be a responsive UI that can even function offline with near-instant intent recognition, fulfilling the Sprint 3 goal of a “Lightning Fast” on-device assistant.
CLI Interface Audit & Optimization
Current State: The CLI (packages/cli) offers a text-based interface to the assistant. It was updated to utilize the new intent classification API before further processing commands. Currently, when a user types a command into the CLI, it sends it to the backend (/api/intent/classify), then prints out the predicted intent and slots with a confidence percentage. After that, it continues with existing processing – which might involve sending the query to the LLM for a full response, or executing a command script, depending on original design. The CLI also has a testing script (npm run test-intent "Your command") to quickly classify sample inputs via the API. Findings & Issues: The CLI integration is straightforward, but there are opportunities to improve or clarify its role:
If the CLI is meant primarily for debugging and development, then its current implementation (just showing classification results) might suffice. In this case, ensure it stays updated with any API changes (for example, if the API returns additional fields like needsConfirmation or new slot types, the CLI could be extended to display those too for completeness).
If the CLI is intended as a full user-facing interface (for power users who prefer terminal), we might consider allowing it to also execute intents. Right now, after classification, it prints the info and then comments “Continue with existing command processing...”. If “existing processing” meant the old behavior of sending the request to an LLM agent, we should reconcile that with the new classified intent. Optimization: We could make the CLI actually perform the action corresponding to the intent:
For example, if intent is create_note, the CLI could save a note locally or call a backend note endpoint.
For send_email, perhaps prompt for email content and send via an API.
However, these would require additional integrations (and storing user creds for email, etc.), which might be out of scope. Alternatively, the CLI could simply call the same execution logic as the mobile but adapted for desktop (if any).
Consistency: As we add offline capabilities, the CLI could also use the local model instead of calling the server. This would make it extremely fast and offline-capable (matching the mobile’s evolution). Node can load TensorFlow.js or an ONNX model as well (tfjs-node or ONNX Runtime for Node.js). We might implement a flag for the CLI like --local to use local inference. This ties into the same logic as mobile: abstract the classification so both CLI and mobile can call either a local function or remote API.
Code Quality: The CLI codebase is likely small. We should still ensure proper error handling (e.g., if the fetch fails or returns an error, catch it and show a message). Logging can be enhanced for debugging (maybe a verbose mode to show raw responses).
Testing: Because the CLI is non-GUI, we can automate tests for it. For instance, a script could run known commands through the CLI and assert on the output (similar to how E2E tests were done in the backend). We already have some test invocation in documentation. We might formalize that into a test suite.
In summary, the CLI should remain a thin but robust client to the backend. Keeping it in sync with the backend’s features (like new intents or fields) is crucial. With potential enhancements, it could become a powerful tool for users or developers to interact with the assistant in environments where a GUI is not available, and it can leverage the same optimized pipelines (including offline models) for consistency.
Voice Input Handling Audit & Optimization
Current State: Voice input is handled in the mobile app via a VoiceIntentService (or similar) that ties together speech recognition and intent processing. The workflow is:
The user activates voice capture (e.g., a microphone button).
The app starts recording via a voiceService.startRecording() (likely using Expo’s Speech API or a plugin).
It then stops recording and obtains a transcription of the audio (voiceService.stopRecording() returning the recognized text).
The transcribed text is sent to apiService.classifyIntent(text) to get the intent result.
If confidence is low (<85%), the service returns an object indicating a correction is needed, so the app can prompt the user (possibly by showing the same Correction UI).
If confidence is high, it proceeds to execute the intent (apiService.executeIntent(intent)) and then uses text-to-speech (voiceService.speak()) to read out the result or confirmation message. The code even formats a spoken response based on the intent (for example, reading back the event details).
In case of any error (recognition failure, classification error), it catches and speaks an apology message.
Findings & Issues: This voice pipeline adds a great user experience, but has complexity:
Speech Recognition Reliability: The quality of voiceService.stopRecording() transcription is crucial. We need to ensure it uses a reliable STT (speech-to-text) engine. Expo’s default may rely on the device’s OS (which is usually good and can be offline for some languages on iOS/Android). If not already, consider allowing the native OS dictation or a library like Google’s Speech API for better accuracy. This may not be directly visible in code, but it’s an external dependency to monitor.
Error and Edge-case Handling: The current implementation handles failure by apologizing via TTS. We should also update the UI if appropriate (perhaps show a toast “Sorry, I couldn't understand”). If the user is speaking and no text is recognized (e.g., silent or mumbling input), ensure stopRecording() returns an empty string or error that we catch and handle (maybe prompt “I didn't catch that, please try again.”). Also, handle quick successive voice commands by queueing or disabling the button while one is in progress to avoid overlapping requests.
Optimization – Wake Word & Continuous Listening: A Nice-to-Have noted is “voice activation” (wake word). This would allow the app to listen in the background for a keyword (e.g., "Hey Assistant") and then start the recording automatically. Implementing this is advanced and can impact battery life. As an optimization plan, we can research existing wake-word detection libraries (like Picovoice Porcupine or Amazon Alexa keyword) that could integrate with React Native. This likely falls beyond the immediate scope, but keeping the code structured to possibly accommodate a background listening service in the future is worthwhile.
Voice and Correction UI Integration: Currently, if a voice intent needs confirmation, the code returns an object with needsCorrection: true. We must ensure that the app responds to this by showing the CorrectionUI to the user with the transcribed text and predicted intent, so they can confirm by voice or touch. Perhaps the app already does this by feeding that object into the same state that triggers the UI. We should test that flow: speak an ambiguous command (e.g., "meeting reminder for tomorrow" which in tests set needsConfirmation true) and see if the CorrectionUI appears with the voice context.
Offline Voice Handling: If the app moves to offline intent classification, STT might still require internet (unless using OS offline models). To fully support offline voice commands, we’d need offline STT as well – which is a bigger challenge. Some Android devices have offline voice typing, and iOS has on-device Siri processing for some languages. As an optimization plan, investigate leveraging the OS capabilities for offline transcription. If not feasible, document that voice commands require connectivity even if intent classification is offline, or consider a hybrid: maybe allow limited set of voice commands offline by using a smaller vocabulary recognizer.
Performance: Ensure that the audio recording and playback do not block the UI. For example, .speak() is likely async – we should not block waiting for it. We might also want to cut off listening if the user is silent for a few seconds (set a max record duration to avoid hang).
Testing: Write integration tests or at least manual test scripts for voice. This is harder to automate, but we can simulate by calling the voice processing function with a preset text (bypassing actual recording) to ensure the logic from transcription to execution holds. Also test on both Android and iOS for any differences in permissions or behavior (microphone permission flows etc. need to be handled).
By refining the voice input pipeline, we will provide a smoother hands-free user experience. Combined with on-device classification, the goal is to eventually allow instant voice interactions (e.g., user says "Remind me to buy milk" and the app responds within a second with confirmation). Robust handling of misrecognitions and guiding the user in corrections will be essential to make this feature feel reliable.
Data Architecture Audit & Optimization
Current State: The app’s data layer uses a Supabase (PostgreSQL) backend for persistence and analytics. Key data structures include:
Intent Predictions Table: Logs each classification result (text, predicted intent, confidence, slots JSON, timestamp, etc.). This is used for monitoring overall performance (e.g., computing accuracy).
Intent Corrections Table: Stores user corrections (original text, what was predicted vs what it was corrected to, slots, etc.). This feeds the learning system so the assistant can improve over time.
Embeddings Table: Planned in Sprint 2, a table to store vector embeddings of user queries for similarity search (using the pgvector extension). Each entry has an embedding (vector), the associated text and intent, confidence, source (e.g., “training” vs “user”), etc.
High-Confidence Cache Table: Not explicitly shown in DDL, but the code intends to upsert very high confidence results to a persistent store, likely another table (e.g., high_confidence_cache) to reuse them across sessions or after restarts.
Intent Registry / Config Files: There are JSON files like intents_registry.json and learning_data_structure.json that likely define the schema of intents and the learning config. These are not runtime data but part of the app’s configuration, ensuring consistency across components.
Findings & Issues: The data schema is generally well thought-out for the learning loop, but some optimizations and concerns:
Indexing and Query Performance: The predictions and corrections tables have indexes on commonly queried fields (e.g., original_text and corrected_intent in corrections). This is good for lookup by text or analysis by intent. The embeddings table uses an ivfflat index on the vector for efficient nearest-neighbor search. We should ensure that the Supabase instance has adequate resources for vector search and that the match_embeddings RPC (if used) is created and tested. As data grows, monitor query performance for the monitoring dashboard: e.g., the current /api/monitoring/accuracy fetches the last 1000 predictions and 100 corrections – this is fine now, but if usage grows, consider summarizing data in rolling metrics or limiting to timeframe.
Data Retention Policy: One risk identified is the storage growth over time. Without a retention plan, the predictions table could become very large (every user query logged). We should implement a policy such as:
Automatically delete or archive predictions older than X days (maybe 30 or 90 days) if they are only needed for recent accuracy tracking.
Or periodically aggregate older data (e.g., store daily accuracy stats in a separate table, then purge detailed logs).
Likewise for embeddings: we might not need to store every single query’s embedding forever. Perhaps keep embeddings for unique queries or those that were corrected, etc. We can also periodically retrain/refresh the vector index with a curated set of embeddings (especially once a stable local model is in use, the embeddings might be less critical).
Supabase/PG can handle a lot of data, but mobile app data tends to be smaller scale; still, it’s good to be proactive.
Consistency and Learning Application: When a correction is submitted, currently the backend likely stores it and possibly updates an in-memory kNN index for immediate effect (the docs suggest a kNN lookup is used for exact matches, implying if the exact phrase comes again, it can pick the corrected intent). We should confirm that:
New corrections update the model/pipeline: e.g., maybe the embedding of that corrected text is added to the embeddings table with the corrected intent, so future similar queries yield a high similarity score to it.
The IntentService.learnFromCorrection() should ideally also update a fast lookup map (for exact text -> corrected intent) in memory, so even before re-training anything, the same text will map to the corrected intent with high confidence.
Ensure the applied_immediately flag in corrections table is set when a correction is used to alter a prediction on the fly.
Multi-user Data: The schema has a user_id in corrections, but not in predictions. If multi-user support is planned (multiple users’ data in one DB), we may want to tag predictions by user as well. This could allow personalized learning (one user’s corrections don’t affect another’s results, if desired). If not needed, we can ignore user_id or use it later for identifying who gave feedback.
Monitoring and Analytics: The monitoring endpoint aggregates accuracy, confidence distribution, common mistakes, etc.. This is extremely useful. To optimize it:
We can perform some of these aggregations in SQL (via a VIEW or a stored procedure) to offload computation to the DB (especially if the dataset grows, doing it in Node might become slow).
The trend calculation for 7-day accuracy is done by filtering in JS; this could be a query grouping by date. But given the small scale, it’s acceptable as is. Just be mindful if usage grows.
For long-term, consider a more interactive dashboard (maybe logging to an analytics service or building a small frontend for the /monitoring data). But that’s an enhancement beyond core functionality.
Security: Ensure that the Supabase credentials are secure and the API does not expose data improperly. The Fastify server likely attaches the Supabase client (fastify.supabase) and uses service role or admin key for full access in backend only. The mobile app uses the backend API and should not directly talk to Supabase (so no leakage of keys – this is correct as implemented). Just double-check no secrets are in the repo and that .env is used.
Backup and Migration: Relying on Supabase means we should trust their backups, but for safety, consider exporting important data (like user corrections) periodically in case of migration or analysis outside the production system. This might not be urgent, but as part of an optimization plan, having data export scripts is useful.
By optimizing the data architecture with retention policies, proper indexing, and ensuring the learning loop is correctly applied, we will maintain a scalable and efficient learning system. This will support the targeted accuracy improvements (90% then 95% as more corrections and embeddings are applied) and keep the system responsive.
LLM and ML Integration Audit & Optimization
Current State: The intelligence of the assistant currently relies on a combination of an LLM and planned machine learning models:
UnifiedLLMService: This service is the interface to a cloud LLM (like OpenAI GPT-4 or similar). It’s used for intent classification (by prompting the model with the user query and perhaps some context of available intents) and for generating training data or test cases automatically. The use of an LLM achieved the initial goal of >85% accuracy immediately on a small set of intents, thanks to the model’s language understanding.
Local ML Model (Planned): To reduce dependency on the LLM (for speed, cost, and offline use), Sprint 3 focuses on integrating a TensorFlow.js or ONNX model that can run on-device or on the backend for inference. The target is to reach ~95% accuracy and have inference <100ms, with the model under 5MB for mobile viability. This likely involves training a smaller classification model (perhaps a DistilBERT or even a custom neural network) on the accumulated data (the generated examples, plus real user data and corrections).
Embeddings & kNN (Added in Sprint 2): As an intermediate step, vector embeddings (using MiniLM or similar) are used to capture semantic similarity. This helps identify if a new query is similar to a past query or known example. The system can then either directly use the intent of the nearest neighbor or use it to augment the LLM prompt. This was aimed to boost accuracy to ~90% by leveraging past knowledge.
Fallback Hierarchy: The final architecture envisions a pipeline where the fastest methods handle the majority of cases: cache → kNN/embeddings → local model, and only if none of those yield a high-confidence result, the system calls the LLM as a fallback. This ensures users rarely feel the slower LLM latency and also reduces API costs, while still maintaining high accuracy by not solely relying on the smaller model for difficult queries.
Findings & Recommendations:
LLM Usage Optimization: We should examine how the LLM is prompted in IntentService. Does it use a few-shot prompt listing possible intents, or a zero-shot classification request? Are we using temperature=0 for deterministic output? All these affect accuracy. Given the 85% accuracy figure, there may be room to improve the prompt or provide more examples to the LLM. However, since the long-term plan is to minimize LLM calls, a balanced approach is to ensure the LLM is only invoked when necessary:
Use the confidence from the local model or similarity search to decide LLM fallback. For example, only call LLM if the local model’s confidence < 0.6 and no similar past query exists, or if the user input is something novel.
When calling the LLM, perhaps incorporate the top k similar examples (from embeddings) into the prompt to guide it (a form of semantic few-shot).
Monitor the LLM performance on edge cases and continually refine prompts (the AI assistance integration section suggests using the LLM to analyze misclassifications, which is clever).
Training the Local Model: To integrate the TF.js model, we need to train it on available data:
Start with the synthetic data generated by the LLM (the intent_training_enhanced.csv with hundreds of examples per intent). Then incorporate real user queries and corrections over time to fine-tune.
Choose a model architecture that balances size and accuracy. A small DistilBERT or MiniLM (with fine-tuning) might achieve high accuracy but could be larger than 5MB. Alternatively, a simpler TF model (e.g., a multi-layer perceptron on top of embeddings) could be very fast and small, but might sacrifice some accuracy. We might consider ONNX quantization or TensorFlow Lite quantization to shrink the model.
Leverage transfer learning: since we already use MiniLM embeddings for kNN, one approach is to continue using those embeddings as features for classification (rather than raw text). That way, the TF.js model doesn’t need a full tokenizer and transformer on-device – it could take a 384-dim embedding (from the same MiniLM) and run a small dense neural network to classify intent. The trade-off is needing to generate that embedding on-device or backend (which could be done via the @xenova/transformers in JS, but that itself has a cost ~100ms). However, given the pipeline, maybe the TF model is a full model that directly does from text to intent.
Regardless, thorough testing and possibly an A/B comparison with LLM on a validation set should guide the model choice. The goal is to not drop below ~95% of LLM’s performance. The plan to do an A/B testing framework in Sprint 2 suggests evaluating the model’s predictions vs LLM to ensure quality before fully switching.
Integration of TF.js Model: Once we have a model, integrate it in both backend and mobile:
Backend: We can use TensorFlow.js in Node, or convert the model to ONNX and use onnxruntime-node for efficiency. The backend can serve as a fallback for older devices that can’t run the model, or as a central point for heavy lifting. However, if the model is small and fast, running on device (mobile) is preferred for offline use.
Mobile: Use tfjs-react-native (which uses expo-gl for GPU acceleration of tfjs) if the model is a tfjs graph. Or, if using ONNX, there might be a React Native ONNX runtime module. We must ensure compatibility (some ops might not be supported on mobile JS; careful model simplification might be needed).
Provide a mechanism to update the model (if we improve it, how to get it to user devices? Possibly via app updates or a download).
Also, consider a scenario where the model is not loaded (first run, or memory constraints) – the app should seamlessly fall back to using the cloud classification to avoid downtime.
Multi-intent & Context (Future): The nice-to-have list includes multi-intent support and context awareness. Our audit should note these are not yet implemented. Multi-intent means parsing commands like "Send an email then create a reminder" in one utterance. This would require splitting the query and possibly making two classifications. The architecture could be extended to detect conjunctions and then process sequentially. Context awareness would allow follow-up questions to omit subject (“reschedule it to next week” referring to the last event). That would require storing conversation state on the backend or device. While these are beyond current scope, our plan can accommodate them by:
Designing the system such that after executing an intent, the relevant info (like the created event details) can be referenced if the next input is related. Possibly keep a short session memory on backend keyed by a user ID or device.
For multi-intents, if the classifier or model returns multiple intents or a composite result, the app could iterate through them. The current data structures do not directly allow multiple intents per query, so this would be an extension when needed.
Continuous Improvement: After Sprint 3, to maintain a “world-class NLP system”, we should set up a process for continuous learning:
Regularly retrain the local model with new data (perhaps when enough new corrections have accumulated).
Monitor accuracy by intent and identify if any intent falls below target, then analyze why (maybe new phrases not covered by training).
Expand the system to new intents carefully by adding training data and possibly new slots in extraction rules. The architecture is flexible to add more intents (just update registry, train model, and implement new execution handlers).
Also consider multi-language support (post-Sprint 3 next steps). Our pipeline could generalize if the LLM or embedding model supports other languages, but a new model or translation step might be needed. We should design the code to handle locale if needed (e.g., separate models per language or a language-detection step before classification).
In summary, the LLM and ML integration plan is on track. Our recommendations emphasize careful integration of the small model to take over the bulk of requests, with the LLM as a safety net. By doing so, the app will drastically improve response time (from 300-500ms down to ~100ms) and work offline, while actually increasing accuracy through layered intelligence. It is crucial to maintain the fallback and continuously feed the learning system with any mistakes the model makes (closing the loop with LLM analysis or developer review) to reach and sustain the 95% accuracy goal.
Conclusion and Next Steps
This full audit has reviewed the backend services, mobile app, CLI, voice handling, data structures, and AI layers of the Personal Assistant app. The codebase is strong in its foundation – modular services, integration of cutting-edge NLP (LLM, embeddings), and a clear vision for improvement. By implementing the optimizations above, we will achieve the following outcomes:
Unified, Efficient Backend Pipeline: A cohesive classification service that smartly layers cache, similarity search, and a fast ML model, with the LLM as an infrequent backup. This maximizes speed and minimizes cost while preserving high accuracy.
Improved Mobile Experience: A mobile app that not only interfaces smoothly with the backend but can also function offline with on-device inference. Users will experience near-instant responses and a slick UI for corrections and voice commands.
Robust Voice and CLI Interfaces: Voice commands will feel natural and responsive, with good error recovery and potential for always-listening activation. The CLI will remain a useful tool for both developers and advanced users, staying in sync with all new features.
Scalable Learning System: The data architecture will support continuous improvement without bogging down the system – employing data retention policies, thorough analytics, and automatic learning from user feedback. The system’s accuracy should climb from 85% to 90% to 95% over the sprints, as planned, and continue upward as more data comes in.
Maintainability and Extensibility: Refactoring to eliminate duplicate logic and using configuration files for intent definitions will make it easier to add new intents or support new platforms/languages. The team handoff will be smoother with comprehensive documentation (which has been started in Sprint 3 with an Intent README and should be updated as needed).
Finally, it’s worth noting that after these optimization steps, the Personal Assistant app will be production-ready – delivering a “world-class NLP system” with 95% accuracy, <100ms response, and self-improving capabilities. The next steps beyond would be to leverage this robust core for adding more user-facing features (proactive suggestions, multi-turn conversations, integration with more services) and scaling to a broader user base. With the solid architecture and optimizations in place, the app is well-prepared for those future expansions.