VoiceCore for iPhone 16 Pro – Implementation Plan
Abstract:This document lays out a production-grade, on-device voice assistant pipeline (speech → intent) specifically optimized for a single user on an iPhone 16 Pro. All components are open-source and tuned for speed, accuracy, and offline privacy. We validate model choices (Whisper ASR + a distilled ~1.5B param LLM), decoding settings, app architecture (Expo with native modules), training/quantization workflows, UX considerations, and metrics. The plan emphasizes low latency (sub-second response), high intent accuracy, and robust JSON output formatting. Extension notes for older devices and multilingual support are included after core specs.

1. Target & Constraints
	•	Device/User: A single English-speaking user on iPhone 16 Pro (A18-class chip, 2025 model). We target the latest hardware for maximum on-device capability. Older iPhones will be considered later.
	•	Privacy: 100% on-device inference. No cloud API calls for speech or NLU. (Network may be used only for optional email fetching in the main app, but VoiceCore itself runs fully offline.)
	•	Latency Goals: Tight end-to-end latency:
	•	ASR partials: ~200ms spacing between interim transcripts.
	•	Final JSON output: < 200 ms (95th percentile) from end of user speech.
	•	Overall voice command to action: < 600 ms (p95) for short utterances.
	•	Reliability: Outputs must be well-formed JSON with deterministic keys. No malformed JSON is acceptable in live use.
	•	Confirmation thresholds: Two confidence cut-offs:
	•	If model confidence ≥ 0.91: auto-execute the intent.
	•	If confidence < 0.81: require user confirmation or present alternatives.
	•	Rationale: industry experience shows raising confirmation threshold near ~0.8 can eliminate false accepts at the cost of more prompts. We choose 0.81/0.91 to balance user trust vs. convenience.

2. System Overview
Pipeline: Mic audio → Whisper ASR (streaming) → Text stabilizer → LLM NLU (JSON output under grammar) → Confidence check (0.81/0.91 gates) → Intent dispatcher → iOS action (Calendar/Reminders/Notes/Email) → (optional TTS feedback)
	•	ASR (Speech-to-Text): Uses OpenAI Whisper run locally via WhisperKit (Core ML) for streaming transcription. The model listens continuously with voice activity detection (VAD) and provides partial transcriptions in real time.
	•	NLU (Intent & Slots): Uses a small LLM (~1–2B params) fine-tuned for intent parsing. It takes the stabilized ASR text and outputs a JSON with intent and slots. Decoding is constrained by a JSON grammar to guarantee valid syntax (the model is only allowed to emit tokens that produce a correct JSON structure).
	•	Policy & gating: The LLM also provides a confidence score in the JSON (policy.confidence). We combine this with ASR confidence to decide whether to auto-execute or confirm (see §8).
	•	Fallback/Rules: If the LLM’s confidence is low or it outputs “intent: none”, a simple rule-based or TF-IDF k-NN matcher can attempt to interpret the command. This prevents user frustration in edge cases but is only a safety net for out-of-distribution inputs.
	•	Device connectors: Once an intent is confirmed, the app invokes native iOS APIs: e.g. create calendar events via EventKit (through Expo’s Calendar module), add reminders, open Apple Notes via URL schemes, read/send email via a (stubbed offline) Gmail module, etc. All these integrations use official iOS frameworks (Calendar/Reminders via expo-calendar, Shortcuts URL scheme for Notes, etc.) to perform the actions on-device.
	•	Optional TTS: For certain confirmations, the system can use iOS AVSpeechSynthesizer to speak back (completely on-device TTS) if needed – for example, reading an email or confirming a scheduled event. This is an enhancement for user experience, not a core pipeline component.
By keeping everything on-device, we ensure zero data leaves the phone (aligning with Apple’s push for on-device ML) and minimize latency associated with network calls. The iPhone 16 Pro’s A18 SoC is powerful enough to handle a medium ASR and a small LLM concurrently (e.g. it can even run a 13 billion parameter model at ~15 tokens/sec entirely on-device, so our lighter models are well within its capability).

3. Optimal Model Choices (Open-Source)
3.1 Speech Recognition (ASR)
	•	Whisper via WhisperKit: We choose Whisper small.en as the primary ASR model, using Argmax’s WhisperKit library for iOS. WhisperKit provides real-time streaming transcription with hardware acceleration (Apple Neural Engine via Core ML). The small.en model (~244 M params) offers a strong accuracy/latency tradeoff for English commands. It should run in real-time on the A18 chip, providing timely partials and final transcripts. In WhisperKit’s benchmarking, even larger models can stream on Apple Silicon, but smaller models are recommended for live usage.
	•	Backup model: We will also bundle Whisper base.en (~74 M params) as a fallback. The base-en model is faster and lighter (for thermally constrained conditions or longer sessions) at the cost of a bit of accuracy. The app can dynamically switch to base.en if it detects prolonged heavy usage or if the device starts to thermal throttle.
	•	Why Whisper? Whisper is state-of-the-art in robust speech recognition and is open-source. Running it on-device via WhisperKit avoids any network latency. WhisperKit supports VAD (voice activity detection) to detect when the user stops speaking, and provides timestamps and partial results in streaming mode. These features let us implement responsive, real-time voice input. (WhisperKit even has a TestFlight demo showcasing live transcription.)
	•	Streaming settings: We use WhisperKit in streaming mode. As the user speaks, partial hypotheses are emitted about every ~250 ms. Once the model stabilizes on a phrase (no change in best prediction for ~0.2 s), that segment is marked “final”. At that point, the text is fed to the NLU. This yields a low-latency pipeline: we don’t wait for the user to finish an entire long sentence – we can start interpreting earlier segments as they stabilize.
	•	Post-processing: We apply light text normalization on the Whisper output: e.g. lowercase the text, restore basic punctuation, and expand a few casual terms or abbreviations (like “tmrw” → “tomorrow”). We do not do heavy text rewriting because we want the raw intent (e.g. “remind me at 5” should remain that phrase for the NLU to parse time). Whisper’s output is already quite clean for English.
	•	Deterministic decoding: We run Whisper in its default greedy mode (temperature 0 or very low) for consistency. We may set beamSize = 1 for speed as recommended. Since the domain is narrow (voice commands) and we’ll fine-tune the NLU to handle slight phrasing differences, deterministic ASR is acceptable and avoids random errors.
	•	Alternative (cross-platform) ASR: If needed, we note that an alternative exists in the form of whisper.rn (a React Native binding of Whisper.cpp). It’s an open-source RN module that also runs Whisper models on-device. For now, we opt for WhisperKit because it’s highly optimized for Apple’s Neural Engine (Core ML models) and suits our single-platform target. But whisper.rn could be used to extend support to Android easily, as it was tested on both iOS and Android devices with good results (tiny/base models achieving real-time on iPhone 13 Pro and Pixel 6).
3.2 NLU (Intent & Slots via LLM)
	•	Student LLM: We use a small distilled LLM (~1–2 billion parameters) that is fine-tuned to output our intents and slots JSON. Recent community models indicate that ~1.5B parameters can achieve remarkable reasoning and NLU performance when distilled from larger models. For example, DeepSeek’s 1.5B model distilled from Qwen-7B (Math) outperforms even GPT-4 variants on certain logic tasks – demonstrating that a well-trained 1–2B model can be highly capable. We will leverage this trend by training a student on our specific task.
	•	Candidate: DeepSeek-R1-Distill-Qwen-1.5B (MIT Licensed) is a strong option. It’s an open-source 1.5B model distilled from a larger Qwen2.5 model, fine-tuned on 800k samples. It excels at reasoning and has been proven on small device scenarios. As an Apache/MIT licensed model, it fits our “open and free” requirement. We can further fine-tune it on our voice command dataset.
	•	Alternate: Llama 3.2 – 1B (if available via Meta/ModelScope). Meta’s Llama series is widely used, and a 1B “Llama-3.2” model has been mentioned in on-device contexts. If an official Llama2/3 small variant is accessible, it could be used as well (with the caution that Meta’s license has some restrictions; Qwen’s Apache license is more permissive). We anticipate similar performance in either case after fine-tuning. In practice, we will evaluate both and choose the one with better accuracy on our intents.
	•	Teacher LLM (for training): To generate high-quality training data and guide the student, we use a larger model (7–8B range) on our MacBook for dataset augmentation and knowledge distillation. For instance, we might use Llama-2 7B or Llama-3.1 8B Instruct as a teacher. The PyTorch Edge library’s torchtune toolkit has a recipe explicitly demonstrating distilling a Llama 3.1–8B into a 1B model, confirming the feasibility of compressing a model while retaining much of its capability. We can follow a similar approach: have the teacher model generate labeled outputs or logits for the student to learn from.
	•	Rationale for ~1–2B size: It is small enough to run under 200 ms on an iPhone16Pro (especially with quantization). For perspective, Apple’s optimizations show an 8B model can reach ~33 tokens/sec on an M1 Mac. An iPhone16’s A18 chip, while less powerful than M1 Max, should handle 1–2B models easily on its Neural Engine/CPU. A 1.5B int4 model should occupy only a few hundred MB of memory and can generate a short JSON (maybe ~20–30 tokens) in well under a second. This meets our latency goal. And thanks to advanced distillation techniques, the intelligence is high relative to its size – modern 1–2B models can carry out complex intent understanding with >90% accuracy given focused training.
	•	Quantization: We will quantize the model to 4-bit weights for deployment. Both the MLC framework and llama.cpp support 4-bit quantization with minimal accuracy loss (approx 3–5% perplexity drop reported for int4 vs fp16). Apple’s own LLM research also applied int4 quantization to reduce memory and improve speed on device. We will use a similar approach (see §7). Quantization is essential to fit the model in RAM and to leverage faster int8/int4 matrix ops on Apple Neural Engine.
	•	Grammar-constrained output: The LLM is configured only to output valid JSON. This is achieved by using a context-free grammar (CFG) during decoding, a feature supported by libraries like llama.cpp (the llama_sample_grammar method) and also by some inference frameworks (Fireworks’ models, LocalAI, etc., have similar JSON schema enforcement). In effect, at each token generation step, any token that would violate the JSON format (as defined in our BNF grammar) is given zero probability. This ensures the model never produces a malformed JSON string, no matter what it “thinks”. It’s a critical feature for reliability. We will implement this either via llama.cpp’s built-in grammar support or using a third-party constrained decoding library. With grammar in place, we can safely use greedy or low-temperature decoding without worry, because even if the model’s natural distribution might have produced an error, the decoder will prevent it. (OpenAI’s APIs don’t expose this, but local inference libraries now do, as evidenced by the llama.cpp update and community interest in grammar-driven generation.)
	•	Decoding params: The model prompt will include a system instruction with the JSON schema and a couple of examples (one-shot or two-shot), to anchor it. Decoding will be greedy or near-greedy (temperature ~0 to 0.2, top-p ~0.9, top-k ~20) but again with grammar as the main constraint. The output is short and factual (just filling slots), so creativity from high temperature is not needed. We prioritize determinism and speed.
	•	Confidence estimation: The LLM’s JSON includes a policy.confidence field. This is meant to be the model’s own estimate (0–1) of how sure it is about the intent/slots. We will calibrate this value (see §6.4) so that it correlates well with real correctness. If the model is unsure, it can also set a needs_confirmation flag in the JSON. Our gating logic will use these signals. Confidence in NLU is tricky; by default, a model’s probability for its top prediction isn’t always a true confidence – hence we will apply temperature scaling post-hoc to make it more meaningful for thresholding.
Note: All model components (Whisper and the chosen LLM) are open-source and free: Whisper’s model and code are MIT licensed, WhisperKit is MIT licensed, Llama and Qwen models are under community licenses (Meta’s terms or Apache-2.0/MIT as noted), and our fine-tuning code will use libraries like HuggingFace Transformers (Apache-2.0) or PyTorch (BSD-style). We avoid any proprietary models or paid APIs.

4. Decoding & Inference Settings
4.1 Whisper ASR Settings
	•	Partial Transcription: Enabled. We request WhisperKit to emit interim results every ~250 ms so the UI can display live captions as the user speaks. Partial results give the user feedback (e.g. they see “remind me to… [listening]”) and also allow the system to start thinking ahead.
	•	Stabilization delay: We choose a short stability window (~120–200 ms). Once a partial hasn’t changed for ~0.2 seconds, we treat it as final for that segment. This usually corresponds to a pause or the end of a phrase. WhisperKit’s built-in VAD can also signal end-of-utterance when a silence >300 ms is detected.
	•	End-of-speech detection: Likely, 300–400 ms of silence after a recognized phrase or a special end token (like a period) will be used to decide the user has finished speaking. This balances responsiveness with not cutting off in the middle of commands.
	•	Language & model: We fix language to English (language: "en") to avoid any lang identification overhead or mistakes. The model is loaded once at app start (either small.en or base.en CoreML model) and kept in memory to handle continual requests.
	•	Beam search vs greedy: We use greedy decoding (beamSize = 1 in WhisperKit config) for speed. Whisper’s accuracy is high enough that beam search yields only marginal gains but would add latency. We also set Whisper’s temperature to 0 (deterministic mode), ensuring repeatable outputs for given audio.
	•	Punctuation and casing: Whisper model itself predicts punctuation and casing. We will accept Whisper’s output as-is for those aspects. Any minor capitalization fixes (like consistently using lowercase for easier downstream parsing) can be done in text post-processing. For example, Whisper might output “Call John at 5 PM.”; we’d lowercase to “call john at 5 pm.” for feeding the LLM. The LLM’s prompt can be designed to handle or ignore punctuation.
	•	Word timestamps (unused): WhisperKit provides word-level timestamps, but we likely don’t need them for this assistant. We focus only on the transcribed text. (If we later want to highlight words as they’re spoken or align confirmation prompts with speech, timestamps could be used, but it’s out of scope for now.)
	•	Resource usage: On iPhone16Pro, the small.en model (~500 MB CoreML) will use ANE and 2–4 CPU cores. We will test to ensure it can run alongside the NLU model without contention. The base.en model (~140 MB) is even lighter. We assume the device can handle it, given that even older iPhones (A15) could run Whisper base in realtime with optimizations. We will monitor memory and adjust (e.g., unload the model if not in use, etc., though in practice the voice feature will likely keep it loaded).
4.2 LLM Decoding for JSON Output
	•	Grammar enforcement: As noted, the LLM’s decoding happens with a JSON grammar constraint. We will define a strict BNF grammar for the output JSON format (see §12.2 for an excerpt). The inference engine (whether llama.cpp or MLC) will be invoked with this grammar. This guarantees syntactic correctness of the output. It essentially prunes the token options at each step to those that keep the JSON valid. We’ll use greedy decoding under this constraint. Practically, this means if the model ever tries to produce a disallowed character (like a stray quote or a key not in the schema), that token is eliminated and the next likely valid token is chosen. The result is always a well-formed JSON string.
	•	Decoding strategy: Greedy or near-greedy. We set temperature to ~0.1 (just to allow a tiny bit of randomness in case multiple slots could be filled) or even 0 for absolute determinism. Beam search is unnecessary because the grammar will handle most structural needs, and the task is straightforward classification/filling. Beam could even be harmful (slower and might explore odd paths).
	•	Token limits: The prompt + output will be short. Prompt (system + example + user command) will be < 150 tokens. Output should be ideally < 50 tokens (our JSON has fixed keys and a few string values). We will enforce a max output length of, say, 256 tokens to be safe, but in practice it’ll likely stop around 50–100 tokens at most. The model will output the closing brace and stop.
	•	Stop criteria: We can use a stop sequence (like }\n) or rely on the grammar (the grammar will naturally end when the JSON is complete). The model is trained to end with a } and no extra text after. We will ensure to capture the output up to the final brace. If using llama.cpp, we can specify stop tokens (e.g. newline after JSON).
	•	Prompt design: The system prompt will explicitly instruct the model: e.g. “You are VoiceCore, an assistant that converts speech commands to JSON. Output only a JSON per this schema and nothing else.” Then we include the JSON schema definition (as a concise description or even in grammar form). We’ll also include a couple of one-shot examples: e.g.
	•	User: “remind me tomorrow at 7 to call John”
	•	Assistant: { "intent": "add_reminder", "slots": { "title": "call John", "datetime_point": "2025-09-02T07:00:00" }, "policy": {"needs_confirmation": false, "confidence": 0.93} }.
This teaches the model the pattern. (We must be cautious to keep the prompt short for speed; possibly we compress the examples or use only one example if the model was fine-tuned heavily.) - LLM Inference Engine: We have two integration paths: 1. Via MLC-LLM: We compile the model to an optimized MLC format with Metal kernels. MLC supports loading a model in iOS and running inference efficiently on GPU/ANE with 4-bit weights. We would call an async function in a native module (e.g. YourMLCModule.infer(prompt, grammar) which returns the JSON string). This approach should maximize speed by using Apple’s Neural Engine int8 capabilities (A17+ support int8 matrix ops on ANE, so int4 can be handled as well). 2. Via llama.cpp (GGUF): Alternatively, use a lightweight llama.cpp build for iOS, perhaps through the llama.rn React Native module[1]. There is a community RN library that wraps llama.cpp for iOS/Android, which might be easier to integrate. It supports GGUF quantized models and likely can be coaxed to use the grammar feature as well (the underlying llama.cpp C++ supports grammar). We have to verify if llama.rn exposes grammar-constrained sampling; if not, we might modify it or write our own native module.
We will aim to use whichever path gives us the best performance and ease of integration. The MLC route promises better use of hardware (GPU/ANE) – Apple’s demo had an 8B model on M1 Max at 33 tokens/s via Core ML GPU[2]. On an iPhone16Pro, a smaller model could potentially hit >50 tokens/s which is excellent. The llama.cpp route might use CPU or limited Metal acceleration; it’s simpler but potentially slower. Since our model is small, even CPU might be sufficient though (e.g. 1.5B at ~8 tokens/s on older chips, possibly much more on A18). We’ll likely prototype both: - First, use llama.rn (to get a quick end-to-end working app with minimal custom native code, leveraging existing open source). - Then, optimize with MLC (replacing the inference backend for speed). - Memory footprint: A 1.5B model 4-bit quantized is roughly 0.5–0.6 GB of data. This will just fit in memory on a high-end iPhone (which typically has 8GB RAM or more). It’s large but acceptable given that it’s the core of our app’s functionality. We will load the model either from app bundle or after downloading on first launch (to keep initial app size smaller – see §11 on build config). - Threading: The LLM inference will run on a separate worker thread (or internally on multiple threads if using llama.cpp which can utilize 4–8 threads). It’s important to not block the UI. Both MLC and llama.cpp allow multi-threading. We’ll configure ~4 threads (since the iPhone has high-performance cores) to reduce latency. - Output verification: After getting the JSON string from the model, we will parse it with a strict JSON parser in code as an extra safety check. If for any reason the JSON is invalid (it shouldn’t be, due to grammar, but as a belt-and-suspenders approach), we will not use it. In such a rare case, we’d either retry once or fall back to the simpler rule-based handling. But again, grammar-constrained generation and our fine-tuning should virtually eliminate malformed output.

5. App Architecture (Expo + Custom Native Modules)
The application will be built in Expo (React Native) to expedite development and allow easier cross-platform expansion. We will use Expo Dev Client with config plugins to include the necessary native modules for Whisper and LLM:
	•	Development workflow: Using Expo Dev Client means we maintain the managed app configuration (app.json/app.config.js) and use EAS Build to create custom native builds that include our native modules. We’ll write Expo config plugins to configure WhisperKit and the LLM module (for permissions, bundling models, etc.). This gives us the best of both worlds: the convenience of Expo (OTA updates, etc.) and the ability to integrate heavy native ML code.
	•	Native Modules:
	•	Whisper ASR module: We have two options:
	•	Write a native module that wraps WhisperKit’s API (start/stop streaming, subscribe to partial/final results). WhisperKit is a Swift package, so we’d integrate it into the iOS project and expose it via the Expo Modules API or RN bridge.
	•	Use the community module whisper.rn (by @mybigday) which is a RN wrapper for whisper.cpp. It provides functions to initialize a model and transcribe audio (even with a new RealtimeTranscriber for streaming). This is appealing as it’s open-source and cross-platform, but since we already favor WhisperKit for iOS, we might not use whisper.rn right now. Still, it’s good to know it exists and we could fall back to it if WhisperKit integration proves too complex.
	•	We will attempt the WhisperKit route for maximum performance on iPhone. The module (@your-org/whisperkit-expo) will handle: - Microphone access (using Expo’s Audio API or directly AVAudioEngine to feed data to WhisperKit). - Starting the transcription pipeline and emitting events to JS for partial and final transcriptions. - Managing model loading: possibly ship the small & base models in the app bundle (given their size ~500MB and 140MB, we need to consider app size limits – or download on first use).
	•	The config plugin for this will add NSMicrophoneUsageDescription, NSSpeechRecognitionUsageDescription to Info.plist, and ensure the model files are included or downloaded.
	•	LLM inference module: Similarly, we have two approaches:
	•	MLC-LLM native module (@your-org/mlc-llm-expo): We embed the MLC runtime or use Core ML. We could use MLC’s C APIs or create a wrapper in Swift/Obj-C that loads the compiled model and runs MLCChatCompletion. If MLC provides a prebuilt iOS framework, we integrate that. The module exposes a method infer(prompt, grammar) that returns the JSON. It will also handle model file management (likely a .mlc model package or .mlmodelc if Core ML).
	•	Use llama.rn: This is a React Native library on npm that wraps llama.cpp for iOS/Android[1]. It might be easier: we just npm install llama.rn and autolink. We’d still need a config plugin if any special setup is required (like copying a GGUF model into the app or enabling certain iOS capabilities). According to its docs, by default it includes an xcframework for llama.cpp and one just needs to call the JS API to load a model file and generate text. We would use this for initial prototyping. We’d need to implement the grammar support – possibly llama.rn doesn’t expose it yet, so we might modify the native code (it’s open-source) to pass a grammar to the llama_sample_grammar function.
	•	Considering time, we might start with llama.rn (quick to integrate) and later switch to a custom MLC module for better speed on device. Both are open-source (llama.rn is MIT licensed on GitHub).
	•	JS Coordinator (Controller logic): In the React Native layer, we’ll have a central module managing the voice interactions:
	•	ASR Streaming: When the user holds a “voice command” button or says a hotword (hotword detection could be an extension – not in core spec), we start recording audio. The Whisper module streams the audio to the model and emits partial transcripts. We display these in the UI (live captions).
	•	On final segment: Once Whisper emits a final segment (or end-of-speech is detected), we accumulate the text (e.g., partials “remind me” + “to call John tomorrow at 5” become full “remind me to call John tomorrow at 5”). We then call the LLM module with this text.
	•	LLM JSON result: The LLM returns a JSON (as string or already parsed object). We parse and validate it against our schema in JS. If it doesn’t parse, we treat it as an error (and likely fallback to a simpler heuristic or ask user to repeat).
	•	Confidence gating: We compute the combined confidence (see §8). If high, we proceed to execution; if low, we prepare a confirmation UI.
	•	Execute or confirm: For auto-execution, we immediately call the appropriate action handler (e.g., create a calendar event via expo-calendar API, or call an internal function that posts a notification, etc.). For confirmation, we present a summary (“Add reminder: call John at 5:00 PM tomorrow?” with [Confirm] [Cancel] buttons). If the range 0.81–0.91 (mid confidence), we might show the filled-in form allowing user to edit before confirming.
	•	After action: Provide user feedback. If confirmed or auto, maybe a small toast “✅ Reminder added.” If cancelled or unrecognized, show a gentle error or re-prompt.
This coordinator will be implemented in TypeScript. We will be mindful of converting all times to proper Date objects, etc., as needed by the native modules (expo-calendar expects JavaScript Date or ISO strings).
	•	iOS Connectors (native functionality via Expo):
	•	Calendar/Reminders: Use expo-calendar which is a wrapper around EventKit. This allows us to create events and reminders. We’ll need to request permissions (Calendar and Reminders permissions) – expo-calendar’s config plugin can set NSCalendarsUsageDescription and NSRemindersUsageDescription in Info.plist. We’ll create events with title, dates, etc., based on slots.
	•	Notes: Apple Notes is not directly exposed via an API, but we can integrate via Siri Shortcuts or a custom URL scheme. A known approach is to use the shortcuts:// URL with x-callback to run a pre-defined shortcut that creates a note. We can include in our app (or instruct the user to install) a personal shortcut that takes input from the app (note title/body) and creates a note. This is a bit of a workaround; however, since this is a tech demo, we might also simply use an alternative like storing notes in a local database or using Expo FileSystem. For demonstration, though, showing it in Apple Notes is nice. We’ll explore using Linking.openURL("shortcuts://x-callback-url/run-shortcut?...") from RN to trigger it.
	•	Email: The spec mentions the main product uses a Gmail provider. For our on-device core, we may implement a dummy email read/send: for example, have some emails cached and just show one. Or integrate with the iOS Mail app via mailto: links (to compose an email) or via the MailKit framework (though that’s iOS 16+ and requires entitlement). Given offline constraint, maybe we skip actual email sending and just simulate it (or only allow composing in Mail app). This part can be modular for the main product to replace.
	•	Other connectors: We ensure the app has necessary URL schemes allowed (tel:// for calling, sms://, facetime://, maps:// etc., if we ever extend it). In Expo, we add these to Info.plist via config plugin (LSApplicationQueriesSchemes entries for the ones we need).
	•	Expo Considerations: We’ll run expo prebuild (or eas build) to generate the iOS project with our plugins. Our config plugins will handle:
	•	Adding the WhisperKit Swift package to the Xcode project (possibly via Podfile if we wrap it, or including the compiled whisper.rn xcframework).
	•	Copying model files into the app bundle (we might store models in Assets.car or as bundled resources since 500MB might be too large; we might instead have them downloaded at first launch to avoid app size limit issues on App Store).
	•	Setting iOS permissions strings in Info.plist (Microphone, Speech, Calendars, Reminders, etc.).
	•	Community modules: By using expo and RN, we also get access to other community packages (for UI, etc.). We will use something like react-native-paper or custom components for a slick UI, and maybe lottie-react-native for a mic animation, etc., but those are details outside the core scope. The key is the architecture supports iterative improvement on the JS side while heavy lifting is done in native modules.
In summary, the architecture is a modular React Native app with a JS orchestrator and native ML modules. This ensures we can update UI/UX and logic via JS (even OTA updates) while still leveraging the full power of native code for ML inference. All parts are open source: Expo SDK (MIT), our custom modules (which we’ll license under MIT), and the ML libs (WhisperKit MIT, llama.cpp MIT).
(By building on the RN ecosystem, we also position ourselves to potentially support Android later by swapping in whisper.rn and llama.rn on that platform with minimal changes.)

6. Training Plan (Customized for Voice Commands)
We will train and fine-tune the NLU model to ensure high accuracy on our specific intents and phrasing, using an open-source dataset and knowledge distillation approach. The ASR model (Whisper) we will use as-is, but we’ll augment our training data with ASR noise.
6.1 Data Collection & Augmentation
	•	Initial intent dataset: We start with the ~1000 example commands and 300 annotated slot examples that the developer already has (per the spec). These include various phrasings for “create event”, “remind me…”, “take a note…”, “send email to X…”, etc., with labeled intent and slot values. We will expand this dataset to cover more variations and edge cases.
	•	Augmentation for ASR output: To make the model robust to speech recognition quirks, we will simulate ASR “errors” and variations:
	•	Remove most punctuation in the input (since spoken language often has no explicit punctuation).
	•	Randomly drop or slur small function words (e.g. “to”, “a”) occasionally.
	•	Homophones: e.g. “two” vs “too” vs “to”; “for” vs “four”; ensure training covers these.
	•	Numeric formats: augment “7” vs “seven” vs “07:00”. We want the model to correctly interpret times or dates whether they come as digits or words. We’ll generate variants like “7 pm”, “7:00 p.m.”, “seven pm”, “at 7”, “at 19:00” etc.
	•	Casual speech: include contractions or colloquial phrases (“tomorrow morning” vs “tomorrow 8am”, “this evening”, “in two hours”, “next Monday” etc.). The model should map these to proper ISO times or durations.
	•	Out-of-domain examples: Include some utterances that map to intent: "none" to teach the model to gracefully handle irrelevant input. For instance, “play me a song” – not one of our intents, so the model should return intent: "none" or low confidence, prompting no action. This helps reduce false positives.
We can use TTS or just text perturbation to create a synthetic “ASR-like” corpus. No actual audio is needed since we simulate ASR text – but we might also run some sample phrases through Whisper to see what typical errors are (Whisper might mistranscribe names or uncommon words, etc. We incorporate those). - Knowledge Distillation Data: To leverage a larger model’s knowledge, we can generate additional training pairs. For example, use GPT-4 or Llama-7B to paraphrase commands and label them. Or simply prompt a larger model: “You are an assistant that converts English to JSON (schema...). Here’s input: ...” and collect its outputs. However, since our schema is custom, a large model won’t know it without prompting – so an alternative is to use our own teacher (Llama 7B or Qwen 7B) and fine-tune it on a small subset to get it aligned to our task, then have it generate more examples. This might be complex; given time, we might stick to augmenting by hand and using the teacher mostly for KD loss (see below). - Dataset size: We aim for a few thousand high-quality training examples after augmentation. The domain is limited, so we don’t need tens of millions of examples – a few thousand could suffice for fine-tuning given the model’s pre-trained knowledge of language. We will ensure all key variations are covered (different ways to say dates/times, different phrasings for each intent). - Validation set: Set aside ~10% of data for validation (with ASR perturbations applied) to evaluate model accuracy on intent classification and slot filling.
6.2 Supervised Fine-Tuning (SFT)
We will fine-tune the chosen LLM (student) on our dataset to teach it to produce the correct JSON in a single shot.
	•	Method: We can use low-rank adaptation (LoRA) to fine-tune to avoid full model training (which would be heavy even at 1.5B). LoRA allows adding a small number of trainable parameters (adapters), and is efficient in VRAM. This way we can fine-tune on a single high-end GPU or even on the MacBook with 16GB RAM (1.5B model might fit in 16-bit with some memory optimization).
	•	Training hyperparameters: (Initial guesses, will tune as needed)
	•	LoRA rank 8 or 16, LoRA alpha 16.
	•	Learning rate ~1e-4, cosine decay schedule, perhaps 2–3 epochs over the dataset (since it’s small, multiple epochs are fine).
	•	Use a bit of label smoothing (5–10%) to avoid overconfidence.
	•	Sequence length: although inputs are short, we set max seq length to 256 to allow for output length and some longer inputs.
	•	Batch size: as large as fits (maybe 16 or 32 sequences per batch on a GPU).
	•	Prompt format during training: We mimic the test-time format. Likely we wrap each example as: <s>[System prompt with JSON schema]</s><s>[User] {command}</s><s>[Assistant] {JSON}</s>. We ensure the model learns to output JSON and nothing extra. We’ll use the same special tokens the base model expects (depending on if it’s a chat model or raw LLM). If it’s a chat-style (like Llama Chat models), we’ll use roles appropriately. If not, a simple delimiter could be used.
	•	Schema anchoring: We include a few exemplar outputs in every training prompt (some approaches call this in-context learning even during fine-tuning). However, a simpler way: prepend a short system message describing the JSON format at the start of each training sample. This helps the model learn the pattern consistently. The last token of the assistant’s output should be the closing brace } and then an end-of-sequence. We want to avoid the model appending irrelevant text.
	•	Training environment: Ideally use an M1/M2 Mac with GPU for training (PyTorch with Metal backend) or use a cloud GPU if needed. Since all models are open, we can use Google Colab or a local Linux with a decent GPU. The dataset is small so training won’t be too long (a few hours at most).
	•	Evaluation during training: We’ll regularly evaluate on the val set by generating JSON for each example and comparing intents and slot values. Our metrics are intent accuracy and slot F1 (for each slot type). We want to ensure >90% on both by the end.
After SFT, we expect the student model to produce nearly perfect JSON on in-domain text input. But we also need it to output a confidence and needs_confirmation. These might not be in the training data explicitly (unless we decide the logic for them). We can heuristically set them in training labels: - For each training example, decide if it should require confirmation. E.g., if an example is ambiguous or an edge case, mark needs_confirmation: true. Most straightforward commands would be false. This can be somewhat arbitrary, but we can define rules: if any slot is uncertain or missing info -> true; otherwise false. We’ll encode that in training so the model learns some criteria. However, the model’s own confidence score might supersede this flag. - The confidence field in training outputs can be set to 1.0 for clearly correct examples, maybe lower for some borderline ones. To not confuse the model, we might initially set everything to e.g. 0.95 in training to indicate high confidence when it knows the answer. The actual calibration will be adjusted later.
6.3 Preference Optimization (DPO/RLHF – future work)
Once the supervised model is good, we can further refine its judgments on confirmation vs. auto. This is akin to reinforcement learning from feedback. Given the time, this is optional, but for completeness:
	•	Using techniques like Direct Preference Optimization (DPO) or Imitative Penalization (IPO), we can fine-tune the model’s outputs to better align confidence scores with actual correctness. For example, gather pairs of outputs: one where the model correctly says needs_confirmation: false vs another where it incorrectly auto-executes when it was unsure. We prefer the one that asked for confirmation in uncertain cases. By feeding such preferences, the model can learn to better set that policy field.
	•	We could simulate user feedback by taking some ambiguous commands and labeling the “preferred” outcome (usually the safer one). However, since our domain is narrow, a simpler calibration (next section) might suffice rather than full RLHF. We note it as an expansion if needed.
6.4 Confidence Calibration
As mentioned, an LLM’s probability estimates often need calibration to be interpreted as true probabilities. We will do a post-training calibration:
	•	Prepare a held-out test set of commands, including some ASR-error simulated ones, that the model hasn’t seen.
	•	Run the model on them to get JSON outputs and also record the model’s raw policy.confidence values (or we could use the probability of the chosen intent vs second-best intent if we had that, but since we force single output, we rely on its self-reported confidence).
	•	Determine which outputs were correct vs incorrect (by comparing to ground truth intent/slots).
	•	Use a calibration technique like temperature scaling: fit a scalar parameter t such that sigmoid(logit/t) best maps to actual accuracy. If our model outputs a confidence c, we find a mapping to calibrated confidence c'. For simplicity, even a linear rescaling or isotonic regression can be used since we have a small data.
	•	Apply this mapping in the app: when we get model_confidence, we transform it to calibrated_confidence before applying the 0.81/0.91 thresholds. This ensures those thresholds correspond to real probabilities of correctness (roughly). For example, if the model tends to be overconfident, the calibration will push the values down so that a 0.9 truly means ~90% chance correct.
	•	Additionally, we incorporate the ASR confidence. We might do a weighted combination as described in §8 rather than pure multiplication (since ASR errors and NLU errors have different profiles).
The calibration step is lightweight and can be done offline and then the parameters (e.g. a scaling factor or lookup table) baked into the app. This addresses the “confidence conundrum” of LLMs and helps avoid too many or too few confirmations.

7. Quantization & Model Export
To run on device with limited memory and maximize speed, we will quantize the LLM model and export it in the appropriate format for our inference library.
7.1 Target Formats
	•	Whisper models: WhisperKit uses Core ML models. We will use the pre-converted Core ML models provided by Argmax (they host Whisper base, small, etc., as Core ML weights). For example, WhisperKitConfig(model: "small-en", modelRepo: "argmaxinc/whisperkit-coreml") can automatically fetch the model. We might instead bundle the .mlmodelc files to work offline entirely. The base-en model is ~500 MB in CoreML compressed form, small is larger (~1.5 GB) so maybe they have a distilled small or we rely on base-en for offline bundling. Argmax did mention a Whisper large-v3 CoreML in their repo, so similarly small and base are likely available. All these are open-source models.
	•	LLM (student) model: We aim for 4-bit weight quantization. Options:
	•	If using MLC-LLM, we will convert the HuggingFace model to MLC format and quantize to their q4f16_1 scheme (which is 4-bit with some group size, and uses float16 for some parts). MLC’s tools will also compile a portion of the model to a binary that runs on Metal. The final artifact might be a folder containing JSON config, .metal library, and quantized weight files (possibly split into chunks for memory).
	•	If using llama.cpp, we will convert to GGUF format at 4-bit (likely use q4_K_M or q4_0 quantization which are supported by llama.cpp for Llama-2 and other models). q4_K_M is one of the latest modes offering good accuracy. There are community scripts to do this conversion from PyTorch or HF model to GGUF. The output is a single file (maybe ~600 MB for 1.5B).
Both formats are free and open. We’ll choose based on integration path. For now, suppose we go with llama.cpp/GGUF for simplicity, then possibly repeat with MLC later if needed. - Quantization workflow: We must measure accuracy after quantization to ensure the model still performs well. 4-bit can sometimes slightly degrade understanding. If we see any concerning drops, we could try 5-bit or 8-bit. But given others’ success in 4-bit (even Apple used int4 for Llama 8B without significant loss), it should be fine. - Model size expectations: ~1.5B 4-bit is around 0.75B * 4 bits = 3 billion bits = ~375 MB (plus some overhead for embeddings etc., likely ~400–500MB). This should load in about a second or two from storage and then mostly reside in RAM. We need to ensure the iPhone has that free (the 16 Pro with 8GB RAM can spare 0.5GB for this given a slim app). - Dual model footprints: Having both Whisper (0.5–1.5GB) and LLM (~0.5GB) loaded concurrently might push memory limits. If needed, we can unload Whisper after getting text (if the user’s utterance is done) to free memory for LLM inference, then reload if another voice input starts. WhisperKit presumably can deallocate the model or we can have it loaded on demand. Alternatively, use base-en Whisper to keep memory low. - Storage of model: Because app bundle size might be a concern (Apple’s over-the-air download limit is 200MB for cell data, but for TestFlight or Ad Hoc it’s fine; App Store might reject extremely large binaries), we plan that the LLM model will be downloaded on first app launch. We’ll ship a small placeholder, and on launch, the app will download the GGUF or MLC model from a URL (maybe hosted on HuggingFace or our server). This can be done with progress indication. After download, it’s stored in device storage (Expo FileSystem or document directory). We verify hash for integrity. Then it’s loaded. Whisper models could also be downloaded similarly, or we include at least the tiny/base model in the binary to have something out of the box. - Versioning: We will version our model (v1, v1.1, etc.) such that if we deliver an improved model, the app can update it.
7.2 MLC Compilation Checklist (if using MLC)
If we choose the MLC route for LLM, the steps are: 1. Convert HF to MLC: Use mlc_llm prepare tools as per their guide. We specify quantization = q4f16_1 (4-bit). Provide our model’s HF path. This yields an MLC model folder. 2. Integrate into Xcode: MLC can either JIT compile on device or we can ahead-of-time compile. For faster startup, we may run the model once on our device to generate the compiled library, then include that library in app assets. Alternatively, use MLC’s TVM compiler with cross-compilation to produce a .metal binary for Apple ANE. Apple’s Core ML approach usually compiles at runtime the first time (as seen in Apple’s blog, first run triggers model compilation to optimize GPU kernels). We can live with a one-time compilation on device (~30 seconds perhaps) on first run. 3. Test on device: We will write a simple iOS test harness (perhaps adapt MLC’s chat demo) to ensure the model can generate text and respect the grammar. MLC might not natively support grammar constraints, so in MLC’s case we might have to implement a token filtering in the generation loop. (This could be non-trivial in their API – if needed, we might lean on llama.cpp for grammar because it has it built-in.) 4. Throughput check: Time a typical prompt (say 50 tokens prompt + need to generate ~30 tokens) and verify it’s under 200 ms. If it’s borderline, consider further optimizations (like reducing context length support or enabling half-precision KV cache, etc.). Ensure the model uses ANE if possible – Apple’s Core ML might offload int8 ops to ANE (A17 and above support int8 on ANE). 5. Memory check: Ensure no spikes that could kill the app. Possibly run with Xcode Instruments to monitor memory and CPU. 6. Fallback plan: If MLC integration is problematic (lack of grammar or any issue), we’ll proceed with llama.cpp which, while possibly a bit slower, is proven and simpler to control. Llama.cpp’s grammar feature will guarantee correctness, and if performance on iPhone is acceptable (which it should be for 1–2B), that may be sufficient.
7.3 llama.cpp / GGUF Integration (alternative)
	•	If using llama.rn or our own RN module around llama.cpp, we will convert our model to GGUF format with the desired quantization (likely using llama.cpp converter or a tool like text-generation-webui).
	•	We then include the .gguf file in the app (or download on first launch). Using llama.rn is straightforward: after adding the library, we do const llama = await LLama.loadModel({ path: 'model.gguf', numThreads:4 }) in JS (exact API tbd by library) and then const out = await llama.generate('prompt text', {grammar: ..., maxTokens: ...}). We will verify this in their documentation.
	•	We have to ensure the Extended Virtual Addressing capability is enabled if we try larger models (the whisper.rn docs mention enabling it for medium/large models on iOS). For 1.5B, it might not be needed, but if we push to 3B it could. This is done by adding an entitlement/capability in Xcode (which our config plugin can handle).
	•	One concern: Apple App Store might not allow apps with downloadable code, and large model binaries sometimes get flagged as they include lots of weight data (some apps have gotten approval though, e.g. offline AI apps). We should be prepared to explain the model is data, not executable code. Since all is on-device and no new code is interpreted, it should be fine.
All quantization and integration steps will be fully reproducible with open tools (HuggingFace transformers, MLC, llama.cpp). We’ll document the exact commands in our repository so others can verify or modify the model themselves.

8. Confidence Unification (ASR + NLU)
To decide when to auto-execute versus confirm, we combine the confidences from the speech recognizer and the language understanding:
	•	ASR confidence: Whisper (especially in Word-Level timestamps mode) provides token-level probability or a log-prob for the transcription. WhisperKit doesn’t directly output a single confidence score, but we can derive one. One approach: take the average log probability of all tokens in the final transcript, exponentiate to get an overall probability-like measure. Alternatively, simpler, use the length-normalized log probability that OpenAI’s Whisper can output (the Python version outputs no_speech_prob and a confidence). We may need to modify WhisperKit to expose this. For now, assume we can get a confidence that the transcription is correct. This catches cases where audio was unclear or model was uncertain. We normalize this to 0–1.
	•	NLU confidence: The LLM’s policy.confidence (after calibration) represents the probability the intent & slots are correct. It’s our primary measure for understanding. However, the LLM might be overconfident if the ASR text itself was garbled (it might confidently misinterpret nonsense).
	•	Fusion: We combine them. A simple linear blend: final_conf = 0.6 * (NLU_conf) + 0.4 * (ASR_conf). This weights the NLU higher (since even a perfectly transcribed but nonsensical command should not auto-execute). The weights 0.6/0.4 are chosen somewhat arbitrarily and can be tuned. Essentially, if ASR was very uncertain, it drags down the final confidence even if NLU was high.
	•	We might also incorporate rule-based conditions: e.g., if ASR confidence is extremely low (<0.5) and NLU still gave something, perhaps always confirm regardless of NLU confidence (because the user’s speech might have been misheard entirely).
	•	Threshold actions:
	•	If final_conf ≥ 0.91: Auto-execute the intent without asking. (The app might still show a small notification “✔ Event added” but not require user tap.)
	•	If final_conf < 0.81: Ask for confirmation explicitly. Possibly say “I heard: [parsed action]. Should I go ahead?” or show two options. If the parsed intent was “none” (no understanding), then instead of confirm, we might reprompt “Sorry, I didn’t get that.”
	•	If in the band 0.81–0.91: We lean towards confirmation but in a lighter way: e.g., show the filled details and maybe a single “Confirm” button (or even allow voice confirmation). This is a UX nuance. For implementation, it’s effectively a confirmation required state.
	•	Example: User mumbles “remind me mmm at 7”. ASR might be uncertain (confidence 0.5) and gives “remind me [unintelligible] at 7”. The LLM might guess an intent (add_reminder) but with low confidence 0.6. Combined final ~0.56 -> below 0.81, so we definitely confirm: “I’m not sure I got that. Did you want a reminder at 7:00?”. Conversely, user says clearly “Remind me at 7pm to call John.” ASR conf ~0.9, NLU conf ~0.95 (it’s straightforward). Combined ~0.93, above 0.91 -> auto-add the reminder, maybe with a quick spoken “OK, I’ve set it.” since we’re very confident.
	•	Confirmation UI/Flow:
	•	We present the parsed intent in a human-friendly way, using the slots. For example, if intent is create_event and slots title “Lunch with Sarah”, datetime “2025-09-10T12:00”, duration “PT1H”, we show a summary like “Event: ‘Lunch with Sarah’ on Sept 10, 12:00 for 60 minutes.” with [Confirm] [Cancel] buttons. The user can tap confirm. We might also allow them to edit (e.g., tap to change title or time if it misheard slightly).
	•	For reminders and notes, similar presentation.
	•	For email sending, because that’s higher stakes (sending content), we might always confirm unless the confidence is extremely high and perhaps it’s a reply scenario. Likely always confirm an email send for safety in this demo.
	•	The thresholds can be adjusted per intent too (maybe stricter for email).
	•	Learning from corrections: If the user manually edits or corrects after a confirmation, we could log that as new training data. That’s outside MVP scope, but worth noting for continuous improvement.
	•	No “open-loop” execution: i.e., we won’t execute low-confidence actions blindly. This gating ensures that if either ASR or NLU is unsure, the user gets to confirm. This approach aligns with best practices noted in industry – set a confirmation threshold to minimize false accepts.
In summary, the combined confidence model will reduce unintended actions, at the cost of occasionally asking for user approval. As usage goes on, we expect the confirmation rate to drop as the system “learns” (we can tweak thresholds or improve model). Our goal is eventually <20% of commands need confirmation (as noted in metrics).

9. User Experience (UX) Considerations
Even though this is a technical spec, the end-user experience drives many decisions:
	•	Activation & input: User activates by pressing a “mic” button in the app (or potentially a wake-word in future). While listening, the app shows a waveform or some animation, and live transcription of what the user is saying appears on screen (like “captioning”). This provides immediate feedback and builds trust (“the device heard me correctly”).
	•	Partial results UI: We display partial transcripts in lighter text, updating in place, and finalize them (maybe turn bold) when Whisper stabilizes them. This real-time feedback loop is crucial for perceived speed.
	•	Intent visualization: As soon as the JSON is produced (which is almost instant after speech), we translate it into a friendly summary for the user. For example, if JSON says intent: add_reminder, title: call John, datetime: 2025-09-02T07:00:00, the UI might show a formatted sentence: “Add a reminder on Sept 2, 7:00 AM to call John.” The important extracted details are highlighted (time, title, etc.). This appears as a confirmation card.
	•	Auto vs confirm UX: If confidence was high (auto-exec), we might briefly show the card with a checkmark and then execute the action, all within maybe 1 second total. Possibly accompanied by a subtle sound or TTS voice feedback: e.g., the phone might speak “Reminder added.” (Using on-device TTS ensures offline and fast – iOS’s AVSpeechSynthesizer can quickly speak short phrases).
	•	If confidence was low, the card might say “Did you mean: [action]?” with a confirm button.
	•	If intermediate, maybe the card is shown but requires a tap to confirm.
	•	Voice confirmation: An enhancement could allow the user to just say “yes” or “no” after a prompt. Implementing this reliably might be tricky (we’d need to listen again and just detect a yes/no). This is not in core scope, but something to consider for future to keep it hands-free.
	•	Undo option: Especially if auto-executing, we should provide an “Undo” button immediately after an action (for a few seconds). E.g., after sending an email or adding an event, a snackbar “Event added [Undo]”. This mitigates any mistakes and gives user a sense of control. Undo would simply delete the created event or not send the email (maybe by holding it for a few seconds before sending).
	•	Multi-turn or single-turn: This VoiceCore is designed for single-shot commands, not ongoing conversation. After executing, it stops listening. If user wants another command, they press the button again. We won’t do follow-up questions except confirmations. However, if an intent was none or unclear, the app could reprompt “I’m sorry, could you rephrase?”.
	•	Edge cases: If user speaks while offline email is requested, we might say “This feature requires network” in the confirmation. But since core is offline, likely skip that.
	•	Visual Design: We can follow a simple chat-like interface: user’s spoken text on one side (maybe as a speech bubble with a mic icon), and the assistant’s interpreted action on the other side (as a JSON preview card or just a descriptive text). This makes the interaction feel like a chat where the user says something and the assistant responds with what it understood. This also provides a history log, which is useful for debugging (“why did it add that event incorrectly?” the user/dev can scroll up and see what was recognized and parsed).
	•	Learning personalization: Possibly allow the user to correct things in-line. E.g., if it consistently misunderstands a name, the user could add to a dictionary. The spec mentioned “Always interpret ‘lunch’ as 60 minutes” – this could be a toggle where if the user corrects an event duration after saying “lunch”, the system notes that maybe “lunch” implies a one-hour event. Implementing this might involve simply storing a small mapping (word “lunch” → duration 60). Due to time, this is a stretch goal, but the architecture (especially using an LLM) would allow injecting such hints in the prompt (like a system message: “Note: treat ‘lunch’ as 60min”). The overall plan supports future growth like this.
The UX aims to make the interaction fast, clear, and with minimal friction. We leverage on-device speed to achieve what feels like an instantaneous assistant. By showing the user what’s happening (live transcript and parsed intent) we keep them in the loop and build trust that the system is doing the right thing. Any uncertainty triggers a confirmation, so the user is always the final judge on potentially risky actions.

10. Metrics & Acceptance Criteria
To ensure the system meets the goals, we will measure several metrics during development (and possibly in a telemetry log for testing):
10.1 Runtime Performance Metrics
	•	ASR Partial Latency: Measure time between user speaking and partial transcript displayed. We target ~<250 ms. We will log p50 (median) and p95 of this delay. This largely depends on WhisperKit’s internals and how fast we process audio chunks. If partials come too slowly, we may adjust frame sizes.
	•	NLU JSON Latency: Measure from the moment a segment of text is finalized to the moment the JSON is produced by the LLM. Target p95 < 200 ms. This includes time to invoke the model and generate output. With our small model and grammar (ensuring immediate termination when JSON done), this should be feasible. We will test many examples on device to ensure this holds even for edge cases.
	•	End-to-End Voice Command Latency: From user start speaking to action executed. Our goal p95 < 600 ms for short utterances (~ one sentence commands). Longer utterances (like a long email body dictated) will take longer obviously (since ASR time scales with speech length), but for typical short commands this is our budget. We’ll test scenarios like “Remind me to stand up in 30 minutes” and measure the total time. If we see slowness, we identify bottlenecks (likely the LLM step or any model loading overhead).
	•	Intent accuracy: On a test set of spoken commands (possibly record ourselves or have a few users test ~100 commands covering all intents), calculate what percent of the time the correct intent was executed (either automatically or after confirmation). We aim for ≥ 90% intent accuracy on this set. Errors could come from ASR mistakes or NLU mistakes.
	•	Slot filling F1: For each slot type (title, datetime_point, etc.), compute precision/recall of extraction. For example, if user said “call John at 5”, did the system correctly get title “call John” and time “5pm today”? We want high F1 (~0.9 or above).
	•	Confirmation rate: What fraction of commands resulted in a confirmation prompt? Ideally this should decrease over time as we adjust thresholds. Initially, it might be 30–40% (since we err on caution), but our aim is to get it < 20% in normal use, meaning 4 out of 5 commands go straight through without bothering the user. We’ll track this during internal testing. Too high means we’re annoying the user; too low (with errors happening) means thresholds are too lax. We will find a sweet spot.
	•	Malformed JSON incidents: We will log if ever the LLM returned invalid JSON. The acceptance criteria is zero such incidents in 1000 trials, thanks to grammar enforcement. This is basically guaranteed by design, but we’ll test a large batch of prompts (including random or tricky ones) to be sure the grammar approach holds. (If any appear, that’s a bug in the grammar or decoding that we fix.)
	•	Thermal/CPU usage: During a 10-minute continuous usage test (like 60 commands spaced out every 10 seconds), verify the device doesn’t overheat or throttle significantly. We can monitor the iOS thermal state if possible. The iPhone 16 Pro should sustain this load, especially if using ANE for most operations which is power-efficient. If we notice warming, we might switch to the smaller Whisper model or reduce some parallelism.
10.2 Acceptance Criteria
We will consider the VoiceCore implementation production-ready when the following are met (on iPhone 16 Pro device):
	•	Latency: 95% of simple voice commands yield an action or confirmation prompt within ≤ 0.6 seconds from end of speech. There should be no noticeable lag. Live partials appear during speech with minimal delay (~0.2s chunks).
	•	Accuracy: On a curated test suite of commands:
	•	Intent classification accuracy ≥ 90%.
	•	Slot extraction F1 ≥ 90%.
	•	This includes correctly handling date/time phrases to ISO format, capturing names in titles or email recipients, etc.
	•	Robustness: Out of 1000 test invocations, 0 malformed JSON outputs. The system should never crash or hang on any voice input (including long or nonsensical input). If input is gibberish, it should safely return intent: none or ask to repeat, rather than produce an invalid result.
	•	Confirmation logic: Achieve a balance where the system rarely auto-executes wrong actions. In internal testing, false accepts (system executed wrong action without user confirmation) should be near zero. It’s okay if it errs on the side of confirming (that just affects UX, not safety).
	•	Resource usage: The app does not consume excessive battery for moderate use (a few minutes of usage doesn’t drain more than a few percent battery – subjective but we can monitor). Also ensure memory usage is within limits (no memory crashes).
	•	Thermals: No thermal shutdown or extreme hot device after extended usage. A slight warmth is expected but it should not throttle CPU down severely. We verify by monitoring if inference speed stays consistent from first command to e.g. 50th command in a session.
	•	All open-source: The final implementation uses only open libraries or our own code. We will have no dependencies on any cloud API or proprietary SDK. The models are our fine-tuned versions of open models (or directly open ones like DeepSeek’s). This is a criterion from the user: we’ll double-check licenses of all included pieces (Whisper MIT, our model MIT, Expo BSD/MIT, etc.) to ensure compliance.
Once all the above are satisfied, we can consider releasing the feature (likely as a TestFlight build or integrated into the main app).
We’ll document these results. For example: “Tested 100 voice commands, 92 correct intent, 8 misinterpreted (with 6 of those caught by confirmation). p95 latency 550ms. No crashes, no JSON errors.” That would signal we hit the mark.

11. Build & Configuration (Expo + EAS)
Building this project involves some custom configuration due to the native modules and large assets:
	•	EAS Build profiles: We will set up eas.json with at least three profiles:
	•	development (Dev Client): includes debug symbols, and includes our native modules (whisper, LLM). This is used for internal testing via Expo Go (Dev Client).
	•	internal (Ad-hoc or TestFlight build): similar to release, but for internal distribution. Might enable some logging/metrics collection.
	•	production (App Store release): optimized, minified JS, etc.
All profiles will reference the same native code, just with different build settings (e.g., debug vs release).
	•	App config (app.config.js): We’ll include the necessary iOS config:
	•	Info.plist entries:
	•	NSMicrophoneUsageDescription: "VoiceCore needs microphone access to listen for voice commands."
	•	NSSpeechRecognitionUsageDescription: "Speech recognition is used on-device to understand your voice commands." (Though we use our own model, Apple guidelines still suggest adding this key if an app performs speech recognition.)
	•	NSCalendarsUsageDescription: "VoiceCore needs access to your calendar to create events."
	•	NSRemindersUsageDescription: "VoiceCore needs access to your reminders to add reminders."
	•	NSContactsUsageDescription: (if we integrate email sending, possibly need contacts to resolve names to emails – but maybe skip for now or say optional).
	•	Potentially NSPhotoLibraryUsageDescription if in future we attach images to notes etc., but not in current scope.
	•	LSApplicationQueriesSchemes: to allow opening certain URLs:
	•	"shortcuts" (for Apple Shortcuts),
	•	"mailto", "tel", "sms", "calshow" (calendar show event),
	•	and any others for completeness (maps, facetime if voice wanted to initiate a call). Example:
	•	ios: {  infoPlist: {    LSApplicationQueriesSchemes: ["shortcuts", "mailto", "tel", "sms", "calshow"]  }}
	•	We’ll also configure expo-calendar via its plugin as shown in Expo docs (to automatically handle permissions).
	•	If using custom config plugins for whisper/LLM, add them in the plugins array, e.g. plugins: ["@your-org/whisperkit-expo-plugin", "@your-org/mlc-llm-expo-plugin"] which will handle additional native setup.
	•	Assets management:
	•	We have large model files. For iOS, one strategy is to put them in Xcode asset catalogs or as bundled files and mark them as not to be thinned. However, embedding a 500MB+ file in the IPA might be an issue for App Store. As noted, better to download on first run. So, we will:
	•	Include Whisper base-en model in the app (since it’s ~140MB, borderline but possibly okay for initial bundle, ensures offline immediate use).
	•	Not include the LLM weights in the IPA. Instead, bundle a small config and then trigger a download from a CDN or HuggingFace. (We must ensure this doesn’t violate any rule – downloading ML model data should be fine as it’s not code execution. Many apps download ML models after install.)
	•	The app will show a one-time message like “Preparing voice model (500 MB)...” with a progress bar. After that, it’s stored and won’t download again unless version updates.
	•	Expo’s updates/asset system: We may need to prevent it from trying to publish these large files. We might mark them in .easignore or something. We also will configure the app to not attempt OTA updates for the native modules (since changes in native require rebuild anyway).
	•	Testing on device: We’ll use TestFlight for external testing. We have to be mindful of Apple’s review – they might be wary of an app that feels like duplicating Siri’s functionality. We might position it as a productivity assistant integrated with calendar/notes. The offline nature is a unique angle (no data leaves device).
	•	If needed for App Store, since they can be picky with speech recognition usage, we might provide an option to use Apple’s Speech API as a fallback (for users who might not want a big download). But since this spec prioritizes offline, we keep our default as offline. Possibly an App Store version could dumb down to use SpeechRecognizer if our model is not downloaded, just to pass review easily (since Apple likes their APIs used for these things). We’ll consider a dual mode to satisfy that, if necessary.
	•	Security & privacy: All audio and text stay on device. We will note that in our privacy policy. Since the app requests mic, calendar, etc., we should clearly explain to user these are to enable the voice features. Perhaps in onboarding, “We process your voice on this device. No audio is sent to any server.” to alleviate privacy concerns.
	•	CI/CD: We might use GitHub Actions with EAS to automate builds. Ensure our config plugins (if custom) run properly in EAS cloud environment (include any needed brew install steps for adding Swift packages, etc., though EAS should handle SwiftPM deps).
	•	Deliverables (from engineering team):
	•	The EAS build profiles and config files as above.
	•	Documentation for how to add the models (maybe a script to download the model and place in the right folder before building, etc.).
	•	Possibly a fallback for development: since downloading 500MB each install is painful, we might allow devs to manually place the model file in the device (via Files app) and have the app look there, to speed up debugging. But that’s an internal detail.
By covering these build details, we ensure that when the time comes to deploy to either internal testers or the App Store, there are no surprises with missing permissions or oversized binary issues. Given all components are open-source, we also intend to open-source our app (or at least these modules) so others can reproduce the setup.

12. Implementation Sketches
To make the plan more concrete, here are some pseudo-code and configuration excerpts.
12.1 Voice Coordinator (TypeScript pseudo-code)
import { startWhisperStream } from '@your/whisperkit-expo';  // hypothetical APIimport { inferJSON } from '@your/mlc-llm-expo';  // hypothetical APIlet partialText = "";let finalText = "";function onVoiceButtonPressed() {  partialText = "";  finalText = "";  // Request mic permission if needed, then:  startWhisperStream({    onPartial: (text) => {      partialText = text;      updateUIText(partialText + "…");  // show partial with ellipsis    },    onFinal: async (text, asrConfidence) => {      // A segment finalized      finalText += (finalText ? " " : "") + text;      updateUIText(finalText);  // show it solid      // If we detect end-of-speech (e.g. text ends with punctuation or a silence gap):      if (endOfSpeech()) {        stopWhisperStream();        const prompt = buildPrompt(finalText);        const jsonStr = await inferJSON(prompt);  // runs LLM, returns JSON string        let parsed;        try {          parsed = JSON.parse(jsonStr);        } catch(e) {          console.error("Malformed JSON!", jsonStr);          // Fallback: simple rule-based parse or ask user to repeat          showError("Sorry, I didn't understand that.");          return;        }        // Combine confidences        const nluConf = parsed.policy.confidence;        const needsConfirm = parsed.policy.needs_confirmation;        const calibratedNLU = calibrateConfidence(nluConf);        const combinedConf = 0.6*calibratedNLU + 0.4*asrConfidence;        // Decide action        if (combinedConf >= 0.91 && !needsConfirm) {          executeAction(parsed.intent, parsed.slots);          showResultCard(parsed, confirmed=true);        } else {          showResultCard(parsed, confirmed=false);          // Wait for user confirm UI interaction        }      }    }  });}function onConfirmButton() {  // User confirmed the action  executeAction(currentParsed.intent, currentParsed.slots);  showConfirmationDone(); // maybe change UI state to executed}function onCancelButton() {  showMessage("Cancelled.");}
The above sketch glosses over many details (like handling multi-segment speech, error cases, etc.) but illustrates the flow: start streaming, collect text, run LLM, parse JSON, combine confidences, branch on threshold.
12.2 JSON Grammar (BNF excerpt)
We define a context-free grammar for the JSON output to enforce the allowed intents and slot value formats. Using a simplified Backus–Naur Form:
grammar VoiceCoreJSON;root  ::= "{" ws "\"intent\"" ws ":" ws intent       ws "," ws "\"slots\"" ws ":" ws slots       ws "," ws "\"policy\"" ws ":" ws policy ws "}";intent   ::= "\"create_event\""       | "\"add_reminder\""       | "\"create_note\""       | "\"read_email\""       | "\"send_email\""       | "\"none\"";slots   ::= "{" ws (slot_entry (ws "," ws slot_entry)*)? ws "}";slot_entry   ::= "\"title\"" ws ":" ws json_string      | "\"datetime_point\"" ws ":" ws json_string_or_null  // ISO 8601 timestamp or offset like +PT5M      | "\"datetime_range\"" ws ":" ws json_string_or_null  // e.g. "2025-09-10T09:00:00/2025-09-10T10:00:00"      | "\"duration_min\"" ws ":" ws json_int_or_null       | "\"email_to\"" ws ":" ws email_array_or_null       | "\"email_subject\"" ws ":" ws json_string_or_null      | "\"email_body\"" ws ":" ws json_string_or_null;policy   ::= "{" ws "\"needs_confirmation\"" ws ":" ws ("true" | "false")      ws "," ws "\"confidence\"" ws ":" ws json_number ws "}";// Terminals for common JSON structures:json_string   ::= "\"" (~["\\] | escape)* "\"";escape ::= "\\\"" | "\\\\" | "\\/" | "\\b" | "\\f" | "\\n" | "\\r" | "\\t";json_int_or_null ::= json_number | "null";json_string_or_null ::= json_string | "null";email_array_or_null ::= "[" ws (json_string (ws "," ws json_string)*)? ws "]" | "null";json_number ::= (digit)+ ( "." (digit)+ )?; digit ::= [0-9];ws ::= [ \t\n\r]*;
(The above grammar ensures the intent is one of the fixed strings, slots keys are fixed and each value is either a string, number, or null as defined. The LLM under this grammar will only produce allowed tokens. We will generate a compiled form of this grammar (possibly in the format llama.cpp expects, which is similar to the above but in their syntax GBNF).)
This grammar explicitly disallows any extra keys or freeform text, so the model’s output stays in structure.
12.3 Prompt Template Example
Below is how we might prompt the model (showing roles as if using a chat format):
[System role]:You are VoiceCore, a voice command interpreter. Convert the user's spoken request into a JSON object with the format:{  "intent": "...",       // one of [create_event, add_reminder, create_note, read_email, send_email, none]  "slots": { ... },      // corresponding slots (title, datetime_point, etc.)  "policy": { "needs_confirmation": false/true, "confidence": 0.0-1.0 }}Follow the schema strictly and output nothing besides the JSON.[Assistant role]: (example){ "intent": "create_event", "slots": { "title": "Lunch with John", "datetime_point": "2025-09-01T12:00:00", "duration_min": 60 }, "policy": { "needs_confirmation": false, "confidence": 0.95 } }[User role]: "remind me tomorrow at 7 to call John"[Assistant role]: 
The model should continue from there with something like:
{ "intent": "add_reminder", "slots": { "title": "call John", "datetime_point": "2025-09-02T07:00:00" }, "policy": { "needs_confirmation": false, "confidence": 0.93 } }
This demonstrates to the model what we expect. During actual inference, we might not include the full example every time if the model is fine-tuned. In fact, if fine-tuning is successful, we might use an even shorter system prompt just reminding format. But showing this as a reference.
(We will ensure the final prompt used is optimized for brevity, since context length costs time. The grammar will handle format, so the prompt can be minimal instructions plus maybe one example.)

13. Extension Options (Post-MVP)
Once the core system works well on iPhone 16 Pro in US English, we can consider expanding:
	•	Older iPhone support: For devices like iPhone XS, 11, 12, etc., which have weaker CPUs and possibly less RAM:
	•	We could include an even smaller ASR model (Whisper tiny.en ~32MB) for those devices to ensure real-time performance. Accuracy will drop, but partial recognition is better than none. Alternatively, use Apple’s built-in Speech for those as a fallback (trading off privacy).
	•	For the LLM, a 1.5B model might be too slow on, say, an A13 chip. In that case, we could distill further to a ~300M parameter model or use a quantization to 3-bit (some frameworks allow 3-bit or mixed 4/3-bit). Or even use an entirely different approach like a fast text classification + slot-filling model (e.g. a small BERT) on older phones if needed.
	•	We can detect device capability at runtime (using expo-device info like totalMemory, etc.) and choose models accordingly. This is an extension where we might maintain a few model variants.
	•	Multilingual & Accents: This spec targeted one user (US English). To handle other languages:
	•	Use Whisper multilingual models (the small or base multilingual). WhisperKit can load multilingual models as well. They are larger and slower (small-multi is ~1.5GB). Possibly not real-time on device unless on newer chips. But for common languages, it might be okay on A18.
	•	Train or prompt the LLM to detect language and output slots appropriately. Or maintain separate models per language. Possibly simpler: restrict initial version to English, but we could allow slight variations (like understanding Spanish for certain slots if needed).
	•	Accents in English: Whisper is pretty robust to accents, so likely fine. If a user has a heavy accent causing issues, not much we can do except perhaps switch to a multilingual model which might have heard more diverse accents.
	•	“Studio-grade” transcription via LAN: An idea: allow the app to offload transcription to a nearby Mac (if on same WiFi) for even higher accuracy (using a large Whisper model on the Mac). This could be a feature for power users – the phone streams audio to the Mac, Mac runs Whisper large, sends text back, then phone continues with LLM. This keeps data within local network (still private). It’s like running a personal server. This is complex and likely unnecessary given how good small Whisper is, but it’s a thought for extension.
	•	Additional Intents & Skills: Once architecture is in place, we can add more voice commands relatively easily by expanding training data and updating the schema:
	•	E.g. “open app X”, “turn on flashlight” – we could integrate with Expo IntentLauncher or custom native calls for device controls.
	•	“what’s the weather” – we’d need a local dataset or an API (would violate offline, unless we have cached weather).
	•	These would require expanding either the LLM or adding a second stage of processing (like connecting to knowledge bases). It’s beyond current scope, but the modular architecture (intent JSON) could integrate with other providers or local knowledge.
	•	Android Support: The Expo approach means a lot of code can be shared. We would need:
	•	Whisper on Android: Argmax has WhisperKit Android (it uses similar approach with NNAPI perhaps). Or use whisper.cpp via whisper.rn which works on Android (with OpenBLAS or GPU via Vulkan maybe).
	•	LLM on Android: MLC has Android support (Vulkan shaders). Llama.rn works on Android as well. So we could fairly easily port by installing those libs for Android and adjusting some file paths. The main work is ensuring performance is acceptable on, say, a Pixel 8. The model might need to be a tad smaller on mid-range Android devices. But it’s doable.
	•	Continuous conversation & memory: Right now, each voice command is independent (stateless). In future, one could imagine the assistant keeping context of previous commands or being able to refer (user: “remind me to call him tomorrow” after previously talking about “John”). That would require coreference resolution and short-term memory in the pipeline (maybe feed last JSON or transcripts as context). This increases complexity significantly (and might push beyond a 1.5B model’s capacity). We likely won’t do this initially, but the framework doesn’t preclude it – one can include previous interactions in the LLM prompt if needed.
	•	Safety & filtering: Since this is offline and user-specific, we’re less concerned about content filtering. But if the user said something that would produce an unsafe action (e.g. “delete all my contacts”), we might want a confirmation regardless of confidence. Or if the user dictates an email that contains some flagged content, maybe warn them. This enters ethical AI territory. Given offline nature, we rely on user’s own discretion. But having some guardrails (like not allowing mass destructive commands without double confirm) is sensible.
In summary, the architecture is flexible for growth. The main challenge scaling to more tasks or languages is obtaining/training models, but since we built everything on open tools, the developer can iterate and improve models as needed (perhaps using new data or community models).

14. Risks & Mitigations
No plan is complete without acknowledging potential pitfalls:
	•	Performance/Latency surprises: It’s possible our on-paper estimates don’t fully pan out. For instance, the LLM might take 400ms instead of 150ms to generate on device, especially if the model ends up being slightly bigger. Mitigation: we have lots of levers – quantize further, use fewer context tokens, optimize prompt, or if desperate, use a smaller model (at cost of accuracy). Since we built in the possibility of using a 1B or even smaller model, we can fall back if needed. Also we ensure to use all hardware (ANE/GPU) to speed up.
	•	Thermal throttling in long use: If the user speaks a lot continuously, the device could heat up. Whisper and the LLM both draw significant power. Mitigations:
	•	Use the base-en Whisper model or even tiny for continuous dictation mode.
	•	Only run the LLM after speech, not during, so not both at exact same time typically.
	•	If device gets hot (we can monitor iOS thermal state), we could temporarily force confirmation mode (reducing computation) or notify user to give it a break.
	•	Also the iPhone16Pro is quite thermally capable, and using ANE for the LLM (if we achieve that) would be power efficient.
	•	Memory pressure: 500MB model + 500MB Whisper could push memory. iOS might kill app if memory usage is too high in background. To mitigate, unload models when not needed (e.g., after executing, free the LLM until next command trigger). Also, using 4-bit helps a lot vs 16-bit.
	•	Accuracy issues with small model: There is a chance a 1.5B distilled model might misclassify occasionally or not generalize to some phrasing. We tried to account with training and distillation. If we find accuracy unsatisfactory, alternatives: try a slightly larger model (3B) if performance allows. Or incorporate a fallback in NLU: e.g., have some heuristic regex patterns for obvious cases and override the LLM if it gets those wrong (like a safety net for simple well-known phrases). For example, a simple date parser could catch if the LLM missed a date.
	•	JSON format complexities: We must ensure our grammar covers all needed syntax. One risk is the model outputting a number in scientific notation or some edge JSON quirk – our parser might fail. We will restrict number formats to plain digits in grammar (as done). We will also thoroughly test.
	•	Model footprint and App Store: The App Store may be unhappy with a huge app size or downloading large files. Mitigation: possibly release this as a separate app (a “VoiceCore Demo”) distributed outside the store or via TestFlight for enthusiasts. If going to App Store, consider making the feature downloadable content (App Store allows >200MB if on WiFi, and apps like some games are several GB so it’s not outright forbidden).
	•	Apple Review concerns: Using non-Apple speech recognition might be fine (plenty of apps use their own models). But we have to clearly explain permissions (“Speech Recognition” permission might confuse reviewers since we aren’t using their API – but we include it because we do recognize speech ourselves). We should emphasize privacy in the description: “All processing is done offline on your device.” – often Apple likes that. If any issue, we have fallback to Apple’s API as mentioned (so we can say it uses Apple’s Speech for certain devices).
	•	User confusion: If the system occasionally misinterprets, users might lose trust. We mitigate by transparency (showing what it heard and what it plans to do) so the user can always intervene. Also by the undo mechanism to revert unintended actions easily.
	•	Maintaining up-to-date data (emails, contacts): If reading emails, the content changes. But since main focus is voice control, not knowledge, we just ensure to fetch latest emails via the Gmail provider if network is available. For offline demo, we’ll use static sample email.
	•	Open-source license compliance: We must include attributions (in app Settings or About, list the licenses for Whisper, Llama, etc.). MIT/Apache require license text inclusion. We will ensure that to avoid legal issues.
	•	Future maintainability: The ML field moves fast. In future, there might be a new model (e.g. a 4B that runs as fast as our 1.5B or an Apple-provided model). Our design is modular, so swapping out the LLM is straightforward as long as it outputs the same JSON. We keep the schema stable and can upgrade model behind the scenes (via new download). This mitigates getting stuck with one model. We should keep an eye on community updates.
Overall, while there are challenges, our approach has multiple fallback options (smaller models, confirmations, etc.) that ensure the system will behave reasonably even under less-than-ideal circumstances. We will test thoroughly, particularly focusing on any scenario where it could do the wrong thing unnoticed.

15. Deliverables & Next Steps
Finally, to wrap up, here’s what we (the development team) will deliver as part of this implementation:
	•	EAS Build Configurations: A complete eas.json and related config to build the app for development, internal testing, and release. This includes proper signing, any necessary build scripts (for model handling), and verification that a fresh clone can be built successfully to an iPhone.
	•	App Config with Permissions: The app.config.js (or app.json) containing all the iOS permission descriptions (Microphone, Speech, Calendars, Reminders, etc.) and required URL schemes so that the app passes the plist checks. We’ll document these strings for review.
	•	Expo Config Plugins: Custom plugins for:
	•	WhisperKit – to add the Swift package or iOS framework, add microphone usage string, and ensure the model files are dealt with. Possibly also to add the Extended Virtual Addressing capability if needed for large models on iOS (as whisper.rn suggests).
	•	MLC/LLM – to handle adding any necessary iOS frameworks (if we bundle a compiled model as a .framework or .mlmodel, etc.), and any special app capabilities. These will be small TypeScript files that EAS runs to tweak the native project. They will be delivered in the project repository.
	•	Model Conversion Scripts: A set of scripts or Jupyter notebooks used to:
	•	Take the teacher model and fine-tune it (LoRA training script).
	•	Perform knowledge distillation (if we did a temperature-driven KD – possibly we skip explicit KD and rely on LoRA + data augmentation).
	•	Merge LoRA weights into base and quantize to 4-bit.
	•	Convert to the final format (GGUF or MLC). For example, a Python script using llama.cpp utils to quantize the HF model to GGUF. We will provide these so the process is reproducible. (They’ll likely reside in a models/ folder or separate repo due to size.)
	•	Calibration Tool: A small Python script that takes model outputs on a test set and computes a calibrated scaling for confidence. This will output something like scale = 0.85 (if we use simple temperature scaling). We’ll then hardcode that in the app’s confidence combiner. Providing the script ensures we can recalibrate easily if model changes.
	•	Automated Testing Harness: At least a basic script to simulate voice commands end-to-end. For example, a Node.js or Python script that takes example phrases, runs them through Whisper (perhaps using whisper.cpp on PC to simulate), then feeds text into the LLM (maybe running the quantized model on desktop via llama.cpp) to verify the JSON output. This isn’t the same as on-device testing, but it will help catch issues in the model/format. We will also include some unit tests for the parsing logic (ensuring our JSON parser accepts the grammar, etc.).
	•	Runtime Logging/Debugging Tools: We might include a hidden debug mode in the app where it shows raw probabilities or allows dumping the audio and intermediate results. This is for development use. For instance, a screen listing last N commands with their ASR confidence and NLU confidence, and whether it auto-executed or confirmed. This helps us fine-tune thresholds. We deliver this as part of the internal build (and remove in production).
	•	Documentation & README: A document (could be in the repo) summarizing how to:
	•	Build the app,
	•	Update the models,
	•	Use the config plugins,
	•	Troubleshoot common issues (e.g., what if model doesn’t load, etc.). Also cover any steps to extend the schema with new intents.
	•	Grammar Definition & Utility: The formal grammar file used for constrained decoding, plus a small wrapper function (maybe in C++ for llama.cpp or in our JS to pass to native) to apply it. We’ll ensure this grammar is easily modifiable if the schema changes.
	•	UI/UX Polish for Confirmation: The actual React Native components for the confirmation dialogs/banners as described. We will provide a consistent design that can be tweaked (maybe using an existing component library for pretty buttons etc.). This includes an icon set (mic icon, confirm/cancel icons, etc., all either custom or from open-source icon sets).
With these deliverables, the person integrating or testing the system will have everything needed to run VoiceCore on their iPhone16Pro immediately. The plan is that after assembling these pieces, the user can install the app, press the voice button, and experience a blazing fast, on-device voice assistant creating reminders, events, notes, etc., with no cloud required.
We will iterate with the user (you) to adjust any details (like if you prefer a different model balance, or if certain phrasing isn’t recognized, we can refine the dataset and retrain quickly). Thanks to everything being on-device and open-source, this system can continue to improve with your feedback, without external dependencies.
By following this implementation plan, we leverage the cutting edge of on-device AI (Whisper and distilled LLMs) to achieve a private, fast voice assistant tailored for iPhone16Pro. The result should align with the goal: incredibly responsive and intelligent voice commands, all running locally – a showcase of what’s now possible with the latest open-source models on consumer devices.

[1] Guide to Running AI Models Locally on Mobile Devices Using React Native and llama.rn | by Volodymyr Snaichuk | Godel Technologies | Medium
https://medium.com/godel-technologies/guide-to-running-ai-models-locally-on-mobile-devices-using-react-native-and-llama-rn-fcd41adbc597
[2] On Device Llama 3.1 with Core ML - Apple Machine Learning Research
https://machinelearning.apple.com/research/core-ml-on-device-llama
